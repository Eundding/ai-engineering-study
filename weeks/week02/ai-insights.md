# Week 02 - AI Engineering 인사이트

각 팀원이 작성한 study-note를 바탕으로 개인별 인사이트와 학습 내용을 정리합니다.

---

## 김성언 (Kim Seong-Eon)

### 핵심 인사이트
- **샘플링의 중요성**: 대부분 간과하는 사실이지만, 샘플링은 모델이 선택 가능한 옵션들 중 어떤 것을 출력으로 선택할지 결정하는 핵심 메커니즘
- **데이터 스케일링의 현실적 한계**: 학습 데이터 부족과 전기 문제라는 두 가지 병목 현상이 AI 모델의 스케일링을 제약

### 독특한 관점 또는 흥미로운 발견
- **AI 생성 데이터의 역설**: 인터넷에 AI 모델이 생성한 데이터가 빠르게 늘어나면서, 결국 AI 학습 데이터는 AI가 생산한 데이터가 될 것이라는 순환 구조의 문제점

### 실무 적용 아이디어
- **샘플링 전략 최적화**: Temperature, Top-k, Top-p 등의 파라미터를 작업 특성에 맞게 조절하여 창의성과 일관성의 균형점 찾기
- **구조화된 출력 설계**: JSON, SQL 등 특정 형식이 필요한 경우, 제약 샘플링과 파인튜닝을 결합한 접근 방식 고려

### 추가 학습이 필요한 부분
- MoE(Mixture of Experts) 구조의 실제 동작 메커니즘
- 새로운 아키텍처(RWKV, SSM, Mamba, Jamba)의 실전 활용 사례

---

## 안태현 (Ahn Tae-Hyun)

### 핵심 인사이트
- **구조화된 출력의 다양한 접근법**: 프롬프팅, 후처리, 제약 샘플링, 파인튜닝 등 5가지 방법이 각각의 장단점을 가지며, 상황에 맞는 선택이 중요
- **환각(Hallucination)의 근본 원인**: 자기 기만과 내부 지식 불일치라는 두 가지 가설로 설명 가능

### 독특한 관점 또는 흥미로운 발견
- **보상 모델의 효율성**: 판단하는 것이 생성하는 것보다 쉽기 때문에, 약한 모델도 더 강한 모델을 판단할 수 있다는 점
- **테스트 시점 연산의 트레이드오프**: 샘플 수를 늘리면 성능이 향상되지만 400개를 넘어서면 검증기를 속일 수 있는 적대적 출력이 증가하여 오히려 성능 감소

### 실무 적용 아이디어
- **Best-of-N 전략 활용**: 중요한 작업에서는 여러 응답을 생성하고 보상 모델로 최적 응답 선택
- **환각 감소 전략**: 간결한 응답 요청, 출처 검색 요구, 사실/반사실 데이터 포함 학습

### 추가 학습이 필요한 부분
- DPO(Direct Preference Optimization)와 RLHF의 실전 비교
- 다양한 샘플링 전략의 실험적 검증 방법

---

## 이은정 (Lee Eun-Jeong)

### 핵심 인사이트
- **사후 학습의 본질**: 사전 학습된 모델이 이미 가지고 있지만 프롬프트만으로 활용하기 어려운 능력을 끌어내는 과정 (전체 연산의 2%만 사용)
- **데이터 품질 vs 양**: 적은 양의 고품질 데이터가 대량의 저품질 데이터보다 더 나은 성능을 낼 수 있음

### 독특한 관점 또는 흥미로운 발견
- **언어별 토큰화 효율성 차이**: 버마어와 힌디어가 영어나 스페인어보다 동일 의미 전달에 훨씬 많은 토큰 필요 → 속도와 비용에 직접적 영향
- **친칠라 스케일링 법칙**: 학습 토큰 수가 모델 크기의 약 20배여야 컴퓨팅 최적

### 실무 적용 아이디어
- **도메인 특화 모델 활용**: 의료, 법률, 코드 등 전문 영역에서는 범용 모델보다 도메인 특화 모델이 효율적
- **비일관성 해결**: 응답 캐싱, 샘플링 변수 고정, 난수 생성기 시드 고정 등의 조합

### 추가 학습이 필요한 부분
- SSM(State Space Model) 계열의 최신 발전 (S4, H3, Mamba)
- 친칠라 법칙의 수정 버전들 (추론 수요 고려 등)

---

## 허채연 (Heo Chae-Yeon)

### 핵심 인사이트
- **샘플링이 저평가되는 이유**: 학습이 모델 성능에 미치는 영향은 잘 알려져 있지만, 샘플링의 영향은 쉽게 간과됨. 적절한 샘플링 전략으로 적은 노력으로 큰 성능 향상 가능
- **Transformer의 핵심 장점**: RNN의 두 가지 한계(최종 은닉 상태만 사용, 순차 처리)를 어텐션 메커니즘으로 해결

### 독특한 관점 또는 흥미로운 발견
- **역스케일링 현상**: 사후학습을 더 많이 할수록 모델이 특정 정치적/종교적 견해를 더 자주 표현하고 자신이 의식을 가졌다고 주장하는 등 사람의 선호도와 멀어질 수 있음
- **컨텍스트 길이의 제약**: 토큰마다 Key와 Value 벡터가 있어 시퀀스가 길어질수록 많은 데이터를 계산/저장해야 함 (단, 파라미터 수에는 영향 없음)

### 실무 적용 아이디어
- **온도 조절 전략**: 창의적 작업(높은 온도)과 일관성 필요 작업(낮은 온도)을 구분하여 적용
- **어텐션 메커니즘 이해**: Query-Key 내적으로 중요도 계산 → Softmax → Value에 적용하는 프로세스 활용

### 추가 학습이 필요한 부분
- 멀티헤드 어텐션의 실제 학습 과정
- 희소 모델(MoE)과 밀집 모델의 성능/비용 트레이드오프

---

## 통합 토론 주제

스터디 세션에서 함께 논의하면 좋을 질문들을 정리했습니다.

### 공통 질문
- **샘플링 전략의 실전 적용**: 각자의 프로젝트에서 Temperature, Top-k, Top-p를 어떻게 조합하여 사용할 수 있을까?
- **환각 해결의 현실성**: 제시된 환각 감소 방법들이 실무에서 얼마나 효과적일까?
- **데이터 품질 vs 양의 균형**: 한정된 자원에서 어떤 선택을 해야 할까?

### 심화 토론
- **친칠라 스케일링 법칙의 한계**: 추론 비용을 고려한 수정 법칙이 실제로 더 나은 선택일까? (Llama의 사례)
- **새로운 아키텍처의 가능성**: RWKV, Mamba, Jamba 등이 Transformer를 대체할 수 있을까?
- **AI 생성 데이터 순환 문제**: AI가 생성한 데이터로 AI를 학습시키는 것의 장기적 영향은?
- **전력 소비 문제**: AI 발전과 환경/에너지 지속가능성을 어떻게 균형 맞출 것인가?

### 다음 주차 연결 포인트
- **모델 평가**: 2장에서 배운 모델링과 샘플링 개념이 3-4장의 평가 방법론과 어떻게 연결될까?
- **환각 측정**: 4장에서 다룰 환각 감지 및 측정 방법론
- **실전 적용**: 구조화된 출력, 테스트 시점 연산 등의 개념을 실제 프로젝트에 어떻게 적용할 것인가?
