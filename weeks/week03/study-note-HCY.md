[3장 velog 링크](https://velog.io/@algorithm_cell/AI-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-3%EC%9E%A5.-%ED%8F%89%EA%B0%80%EB%B0%A9%EB%B2%95%EB%A1%A0)

[4장 velog 링크](https://velog.io/@algorithm_cell/AI-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-4%EC%9E%A5.-AI-%EC%8B%9C%EC%8A%A4%ED%85%9C-%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0)

---

AI 애플리케이션을 개발하는데 있어서 큰 난관은 **평가**이다. 

평가의 목적은 **위험을 줄이고 기회를 발견**하는 것이다.

따라서 이번 장에서는, 개방형 모델 평가에 사용되는 다양한 방법과 작동 원리 그리고 한계에 대해 다룬다.


# 1) 파운데이션 모델 평가의 어려움

파운데이션 모델이 전통적인 ML 모델보다 평가하기 어려운 이유는 

**1. AI 모델이 똑똑해질수록 평가가 더 어려워진다.**

- AI는 그럴듯한 대답을 잘 내놓는다. 따라서 이제는 사실확인, 추론 심지어 전문 지식까지 활용해서 정보가 맞는지 확인해야한다.

**2. 파운데이션 모델의 개방형 특성으로 인해, 정답을 기준으로 성능을 평가하는 방식이 더 이상 유효하지 않다.**

- 개방형 작업에서는 하나의 입력에 대해서도 여러 개의 정답이 잇을 수 있기 때문에 이와 비교할 수 있는 완벽한 정답 목록을 만드는 것은 불가능하다.

**3. 파운데이션 모델은 블랙박스로 취급되기 때문이다.**

- 모델 제공업체가 모델의 세부 사항을 공개하지 않거나, AI 애플리케이션 개발자가 이를 이해할 전문 지식이 부족하기 때문이다.
- 학습과정, 학습 데이터 등과 같은 세부 사항을 모르기 때문에, 모델의 **출력 결과만 보고** 평가를 진행해야한다.

**4. 벤츠마크 모델들이 파운데이션 모델을 판단하기 부적절하다.**

- 파운데이션 모델의 성능이 빠르게 향상되면서, 기존 벤치마크들의 최고 점수에 도달하게 되었다.

**5. 범용 모델의 평가 범위가 확장되었다.**

- 작업 특화 모델에서 평가는 학습된 작업에 대한 모델의 성능을 측정하면 되지만, 범용 모델에서의 평가는 모델이 수행할 수 있는 새로운 작업을 발견하는 것도 포함되기 때문이다.

![](https://velog.velcdn.com/images/algorithm_cell/post/dca2ceab-db2d-47dc-acb2-c6d2117143ff/image.png) | ![](https://velog.velcdn.com/images/algorithm_cell/post/5894f35b-2fa1-4070-91ff-aca060be299d/image.png)
---|---|

- 평가 저장소가 계속해서 늘어나는 추세임에도 여전히 오픈소스 측면에서 뒤쳐지는 모습을 볼 수 있다. 


---

# 2) 언어 모델링 지표 이해하기

파운데이션 모델은 언어모델에서 발전했고, 여전히 핵심 구성 요소로 활용되고 있기 때문에 언어 모델링 지표를 이해하는 것이 애플리케이션의 성능을 이해하는데 도움이 될 수 있다.

대부분의 자기회귀 언어 모델을 **교차 엔트로피**나 **퍼플렉시티**를 사용해 학습된다. 추가적으로 **문자당 비트(BPC), 바이트당 비트(BPB)** 도사용되고, 이는 교차 엔트로피의 변형이다.

언어 모델은 **학습 데이터의 분포를 학습한다.** 따라서 모델이 더 잘 학습할수록, 학습 데이터에서 다음에 올 것을 더 잘 예측할 수 있고 **학습 교차 엔트로피 및 4가지의 지표 값**은 낮아진다.


## 2-1. 엔트로피 (H)

엔트로피 = **토큰이 평균적으로 얼마나 많은 정보를 담고 있는지**를 측정하기 때문에, 엔트로피가 높을수록 **토큰을 표현하는데 더 많은 비트가 필요하다**


<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/06e62444-b8c0-4d69-b0ac-501c3cb3337d/image.png" />
<p align="center">왼쪽) 토큰 2개, 오른쪽) 토큰 4개</p>

- 왼쪽은 토큰이 2개이므로 1개의 비트만 필요로 하지만, 위/아래 두개의 정보만 가진다.
- 오른쪽은 토큰이 4개이므로 2개의 비트가 필요하지만, 더 구체적인 위치를 가진다. 

직관적으로, 엔트로피는 언어에서 다음에 올 것을 예측하기가 얼마나 어려운지를 보여준다.
언어의 엔트로피가 낮다는 것은 토큰이 담고 있는 정보가 적다는 것이고, 이는 다음에 올 것을 **예측하기 쉽다**는 것을 의미한다. (적은 것 중에 하나를 고르는게 더 쉽기 때문이다.)


## 2-2. 교차 엔트로피 (H)

**교차 엔트로피**는 언어 모델이 데이터셋의 내용을 얼마나 예측하기 어려워하는지를 보여주는 지표이다.

언어 모델은 학습 데이터에 대한 교차 엔트로피를 최소화하도록 학습된다.

만약, 언어 모델이 학습 데이터를 정확하게 학습한다면 **모델의 교차 엔트로피는 학습 데이터의 엔트로피와 일치한다.**

### 교차 엔트로피는 2가지 특성에 따라 달라진다.

**1. 학습 데이터의 예측 가능성 (== 학습 데이터의 엔트로피로 측정됨)**

**2. 언어 모델이 파악한 분포가 학습 데이터의 실제 분포와 얼마나 다른지**


> 학습 데이터의 실제분포 : P, 언어 모델이 학습한 분포 : Q 라고 하면
> 
> - 학습 데이터의 엔트로피는 $H(P)$다.
>
> - $Q$와 $P$가 얼마나 다른지는 쿨백-라이블러(KL) 발산으로 측정할 수 있으며, 수학적으로 $D_{KL}(P||Q)$로 표현된다.
>
> - 따라서 학습 데이터에 대한 모델의 교차 엔트로피는 다음과 같다.
>
> <div align="center">
  <img src="https://latex.codecogs.com/svg.latex?H(P,Q)%20=%20H(P)%20+%20D_{KL}(P||Q)" />
</div>
>
> - 교차 엔트로피는 비대칭적이다. P에 대한 Q의 교차 엔트로피 H(P,Q)는 Q에 대한 P의 교차 엔트로피 H(Q,P)는 서로 다른 값을 가진다.

## 2-3. BPC, BPB

- 엔트로피와 교체 엔트로피의 단위는 **bit**다.
- 만약 언어 모델의 교차 엔트로피가 6비트라면, 각 토큰을 표현하는데 6bit가 필요하다는 것이다.

- 언어 모델의 BPB가 3.43이면, 원본 1바이트(8bit)을 3.43bit으로 표현할 수 있다는 것이고, 이는 언어모델이 원본 학습 텍스트를 절반 이하로 압축할 수 있다는 뜻이다.


## 2-4. 퍼플렉시티 (PPL) *perplexity*

**퍼플렉시티**는 엔트로피와 교차 엔트로피의 지수 함수다.


- $$PPL(P) = 2^{H(P)}$$

- $$PPL(P,Q) = 2^{H(P,Q)}$$

**nat** : 퍼플렉시티와 비슷하지만, 밑을 자연상수(e)로 사용한다

- $$PPL(P,Q) = e^{H(P,Q)}$$


퍼플렉시티는 다음 토큰을 예측할 때의 **불확실성**을 측정한다. 불확실성이 높을수록, **다음 토큰으로 가능한 선택지가 많다**는 것이다.

따라서,

**1. 구조화된 데이터일수록 퍼플렉시티가 낮다.**

- ex) html 코드가 일상적인 텍스트보다 퍼플렉시티가 낮을 것이다.

**2. 어휘(vocab)크기가 작을수록 퍼플렉시티가 낮다.**

- ex) 영어 알파벳 개수 데이터셋, 영어 단어 데이터셋을 비교하면, 다음 문자를 예측하는 것이 다음 단어를 예측하는 것보다 퍼플렉시티가 낮을 것이다.

**3. 컨텍스트 길이가 길수록 퍼플렉시티가 낮다**

- 모델이 볼 수 있는 컨텍스트가 많을수록 다음 토큰 예측할 때 불확실성이 줄어든다.


- 퍼플렉시티는 모델이 그 텍스트를 예측하기 얼마나 어려운지를 측정한다. 따라서, **학습 중에 본 적이 있고 기억한 텍스트**에 대해서는 **퍼플렉시티**가 가장 낮게 나타나기 때문에, **어떤 텍스트가 모델 학습 데이터에 포함되었는지 탐지**하는데 사용할 수 있다.

> **단, 퍼플렉시티는 SFT와 RLHF 같은 기법을 통해 사후 학습된 모델을 평가하는 데는 적한 지표가 아닐 수 있다.** 
>
> 특정 작업을 잘 수행하도록 가르치게 되면, 다음 토큰 예측 능력은 떨어질 수 있기 때문이다. 언어 모델은 보통 사후 학습 후에 퍼플렉시티가 높아진다.

---

# 3. 정확한 평가

- 정확한 점수를 산출하는 2가지 평가 방식에 대해 알아보자

## 3-1. 기능적 정확성

- 시스템이 의도한 기능을 제대로 수행하는지 평가하는 것

- 측정을 자동화하는 것은 쉽지 않지만 자동화할 수 있는 작업 유형이 있다. 
 
	
    1. **코드 생성**
    2. **게임 봇**

## 3.2. 참조 데이터의 유사도 측정

- 기능적 정확성으로 자동 평가할 수 없다면 AI의 출력을 참조 데이터와 비교해 평가해야한다.

- 참조 데이터는 (입력, 참조 응답) 형식을 따르고, 하나의 입력에는 여러개의 참조 응답이 있을 수 있다.

- 참조가 필요한 평가 지표는 **참조 기반 지표**, 그렇지 않으면 **참조 없는 지표**라고 한다.

참조 데이터가 필요하다면, **참조 데이터를 얼마나 빨리 만들 수 있는지가 곧 성능의 척도가 된다.**

참조데이터는 AI, 인간에 의해 생성될 수 있다. 사람이 처음부터 생성하는 것보다 AI가 생성한 데이터를 사람이 검증하는 과정이 시간이 덜 들기 떄문에 AI를 활용한다.

마찬가지로, 생성된 응답과 참조 응답간 유사도를 측정하는 것 역시 사람 평가자나 **AI 평가자**(3-4에서 다룬다)가 할 수 있다.

이를 측정하는 지표에는 4가지가 있다.


**1. 비교 : 평가자에게 두 텍스트가 같은지 판단하도록 요청**

**2. 정확한 일치 : 생성된 응답이 참조 응답 중 하나와 정확히 일치하는지 여부**

- 짧고 정확한 답을 가질 때 주로 사용 ex) 퀴즈, 수학문제 

**3. 어휘적 유사도 : 생성된 응답이 참조 응답과 얼마나 비슷해 보이는지**

- 각 텍스트를 더 작은 토큰으로 나누어서 비교 

- 의미가 비슷한 것에 대해서는 판단할 수 없음

**방법**
1. 공통으로 가진 토큰 수를 세는 방식으로 측정하는 방식
2. 근사 문자열 매칭 (퍼지 매칭)

- 한 텍스트를 다른 텍스트로 바꾸는 데 필요한 편집 횟수(**편집 거리**)

	
    - 편집 연산
      1. 삭제
      2. 삽입
      3. 대체

3. n-gram 유사도

- 연속된 시퀀스인 N-gram의 겹침을 기준으로 측정한다.

- **지표** : BLEU, ROUGE,METEOR++, TER, CIDEr

- **벤치마크**  : WMT, COCO Captions, GEMv2


<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/ec346805-3a75-47a7-929e-1cc1120c3de3/image.png" />
<p align="center">푸유 모델에서 캡션의 한계로 낮은 점수를 받은 예</p>

- **단점** : 포괄적인 참조 응답 세트를 만들어야한다. 비슷한 응답이 참조 세트에 없다면 좋은 응답이여도 낮은 성능을 보인다.

**4. 의미적 유사도 : 생성된 응답이 의미에서 참조 응답과 얼마나 가까운지**

비슷하게 생겨도 의미적으로 다른 문장일 수 있고, 다르게 생겨도 의미적으로 비슷한 문장이 있다.

>**활용**
>
>- 검색과 서치 : 질의와 유사한 항목 찾기
>
- 순위 매기기 : 질이에 대한 유사도를 기준으로 순위 매기기
>
>- 군집화 
>
>- 이상 탐지 : 유사도가 낮은 항목을 탐지
>
>- 데이터 중복 제거 

의미적 유사도를 이해하기 위해서는 먼저 **임베딩**에 대해 알아야한다.

### 임베딩

컴퓨터는 숫자로 작동하기 때문에 모델은 입력을 **숫자 표현**으로 변환해야한다.
**임베딩 vector**은 원본 데이터의 의미를 담는 숫자표현이다. 
텍스트 뿐만 아니라, 모든 유형의 데이터가 임베딩으로 표현될 수 있다.


ex) the cat sits on a mat -> [0.11, 0.002, 0.54, .. ]
    이러한 임베딩 벡터의 사이즈는 보통 100~1000 사이다.

</br>

그렇다면 **임베딩벡터를 만드는 모델이 필요할 것이다.** (BERT, CLIP, Sentence Transformers)
![](https://velog.velcdn.com/images/algorithm_cell/post/45774e24-3d71-4536-93b5-e5af7eb9ba4f/image.png)

임베딩 알고리즘의 목표는 원본 데이터의 본질을 담아내는 임베딩을 만드는 것이고, 임베딩의 품질은 **해당 작업의 유용성**이나, 코사인 유사도 등의 지표로 측정할 수 있다.

</br>

현재는, 서로 다른 유형의 데이터에 대한 통합 임베딩에 대한 연구가 활성화 되고 있다.
이미지와 텍스트를 하나의 통합 임베딩 공간으로 매핑할 수 있는 **CLIP**가 대표적이다.


<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/a6cfc402-0eba-4df7-8c4e-f94b8e03aedf/image.png" />
<p align="center">CLIP 아키텍쳐</p>



이제 다시 의미적 유사도의 작동 방식을 보자.

각 텍스트를 **임베딩**으로 변환하고, 두 임베딩 간의 유사도를 **코사인 유사도**를 통해 구한다.
유사도 점수는 -1 ~ 1로 나타나고 1에 가까울 수록 의미적 유사도가 높다.

> $A$를 생성된 응답의 임베딩이라 하고, $B$를 참조 응답의 임베딩이라 하자. >$A$와 $B$ 사이의 코사인 유사도는 다음과 같이 계산된다.
>
> $$
>\text{Cosine Similarity} = \frac{A \cdot B}{\|A\| \|B\|}
>$$
>
>* $A \cdot B$는 $A$와 $B$의 내적
>* $\|A\|$는 $A$의 유클리드 노름($L^2$ 노름)이다.
>  * 예: 만약 $A$가 $[0.11, 0.02, 0.54]$라면,
>
>  $$
  \|A\| = \sqrt{0.11^2 + 0.02^2 + 0.54^2}
>  $$

---

# 4) AI 평가자

AI 평가자는 사람에 비해 빠르고, 사용하기 쉬우며, 비용도 저렴하다. 
참조 데이터 없이도 작동할 수 있고, 실제 서비스 환경에서도 활용가능하다.

AI는 응답을 평가할 뿐만 아니라 자신의 결정을 설명할 수도 있는데, 이는 평가 결과를 검토하고 싶을 때 특히 유용하다.

### 사용 방법

AI 평가자를 만들기 위해서는 **프롬프트**를 잘 작성해주면 된다. 

> 일반적으로 평가자 프롬프트는 다음 사항을 명확하게 설명해야한다.
> 1. **모델이 수행할 작업** (예: 생성된 응답과 질의 간의 관련성 평가)
> 
>2. **모델을 평가할 때 따라야 할 기준** (예: "생성된 응답이 기준 응답에 따라 주어진 질의를 충분히 해결하는 정보를 포함하는지 판단하는 데 두어야 한다.") 지시가 더 자세할수록 좋다.
>
>
>3. **점수 체계**. 다음 중 하나의 방식으로 점수를 매길 수 있다.
>  * **A. 분류:** 좋음/나쁨, 관련됨/관련 없음/중립 등
>  * **B. 이산적인 숫자 값:** 1~5점. 이는 각 클래스를 의미적으로 해석하는 대신 숫자로 구분하는 특수한 분류 방식이다.
>   * **C. 연속적인 숫자 값:** 0과 1 사이의 값. 유사성 정도를 평가할 때 사용한다.

- 언어 모델은 일반적으로 숫자보다 텍스트를 더 잘 다룬다. 

- 경험적으로 이산 점수의 범위가 넓을수록 모델의 성능이 더 나빠진다. 

- 프롬프트에 예시가 포함된 경우에 성능이 더 좋게 나타난다. 



<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/21e33f26-829f-4472-b032-a07c621bdbc2/image.png" />
<p align="center">평가 과정</p>

### 활용

**범용 모델을 평가자로 활용하는 경우**

- AI 평가자는 평가받는 모델보다 더 강력하거나, 약하거나, 비슷할 수 있다.

	- 강력한 모델을 평가자로 쓰면 추가적으로 **1. 가장 강력한 모델을 평가할만한 평가자를 찾을 수 없고, 2. 어떤 모델이 강력한지 판단할 평가 방법이 필요하다.**
   

**평가에 특화된 모델**

1. 응답 자체의 품질을 독립적으로 평가할 때 : **보상모델**

- (프롬프트, 응답) 쌍을 받고 그 응답이 얼마나 좋인지 점수를 매긴다.
	
    - ex) 구글의 "캐피 *Cappy*"는 응답이 얼마나 정확한지에 대해 0~1 사이 점수를 출력한다.

2. 생성된 응답을 참조 응답과 비교해 같은 지 평가할 때 : **참조 기반 평가자**

- 하나 이상의 참조 응답을 기준으로 생성된 응답을 평가한다.

- 유사도점수, 품질 점수를 출력할 수 있다.

3. 생성된 응답 중 **어느 것이 더 나은지, 또는 사용자가 어느 것을 더 선호할지** 예측하기
 : **선호도 모델**

- (프롬프트, 응답 1, 응답 2)를 입력으로 받아 어떤 응답이 나은 지 출력한다.

- 사람의 선호도를 예측한다는 것은 많은 가능성을 열어주기 때문에 가장 기대되는 방향이다.
	
   - 사람의 선호도를 정확히 예측하는 모델이 있으면 전반적인 평가가 수월해지고, 모델을 더 안전하게 사용할 수 있기 때문이다.
    
  	
  ![](https://velog.velcdn.com/images/algorithm_cell/post/01bb2c7f-da8b-4d14-9f8a-f777ecd59571/image.png)
 


### 한계

**1. 비일관성**

- 확률적이기 대문에 한 가지의 지시로도 평가마다 다른 점수가 나올 수 있다.

	- 일관성을 높일 수는 있지만, 높은 일관성이 반드시 높은 정확도와 연결되는 것은 아니다. 이는 일관되게 실수를 할 수 있다는 이야기로도 해석할 수 있다.

**2. 평가 기준의 모호성**

- AI 평가자의 지표를 표준화되어 있지 않기 때문에 잘못 해석하고 오용할 가능성이 높다.
	
    - ex) MLflow, Ragas, LlamaIndex 에서는 **충실성**이라는 내장기준을 가지고 있지만, 이에 대한 지시와 점수 체계가 모도 다르다.
    
    ![](https://velog.velcdn.com/images/algorithm_cell/post/026b619d-5bb0-4dcf-94aa-0b37d09d37fb/image.png)


**3. 비용과 지연 시간 증가**

- 강력한 모델로 응답을 평가하거나, 지시 사항을 길게 하면 비용이 증가하게 된다.

- 운영 파이프라인에 AI 평가자를 구현하면 사용자가 기다리게 되는 **지연 시간**이 늘어날 수 있다.

**4. AI 평가자의 편향**

- AI 모델마다 서로 다른 편향을 보인다.

- AI 평가자는 **자기 편향** 즉, 자신이 생성한 응답을 선호하는 경향을 보인다.

- AI 평가자는 **첫 위치 편향** 즉, 여러 선택지 중에 첫 번째 응답을 선호하는 경향을 보인다.

- AI 평가자는 **장황성 편향** 즉, 품질과 관계없이 더 긴 응답을 선호하는 경향을 보인다.

**5. 개인정보 보호와 지적 개산권 제약**


---

# 5) 비교 평가를 통해 모델 순위 정하기



## 비교 평가의 장점

**1. 두 출력을 비교하는 것이 각 출력에 구체적인 점수를 매기는 것보다 쉽다.**

**2. 사람의 선호도를 파악하는 것을 목표로 하기 때문에, 새롭고 강력한 모델이 등장해도 포화 상태에 도달하지 않는다.**


## 비교 평가의 해결 과제

### 1. 확장성 병목

- 비교 평가는 많은 데이터가 필요하다. 비교할 모델 쌍의 수는 **모델 수의 제곱에 비례해 증가한다.**

- AI 모델에서는 **전이성 가정** 이 성립하는지가 불분명하다. 
**전이성 가정* : A>B이고 B>C이면, A>C이다.
    
### 2. 표준화와 품질 관리의 부재

- 표준화와 품질 관리를 강제하기 어렵다.

> 예시로 LMSYS 챗봇 아레나를 보면
> 누구나 웹사이트에 접속해 프롬프트를 입력하면, 두 모델에서 나온 응답을 각각 받을 수 있고, 더 나은 것에 투표할 수 있다.
>
> **문제점**
>- 인터넷에 접속하는 사람 누구나 평가를 할 수 있고, 기준이 없다.
> 
>  - **크라우드소싱** 방식의 비교는 실제 업무 환경이 아닌 곳에서 모델을 평가하기 때문에, 실제 사용 환경에 대한 이해가 부족하다.
> 
>**해결을 위한 접근 방법**
> 이 경우에서 표준화를 강제하기 위해서는 정해진 프롬프트만을 사용하게 하거나, 신뢰할 수 있는 평가자만 사용하는 것이다.
>
> ** 해당 접근 방벙이 야기할 수 있는 문제점**
> 하지만, 이는 다양성을 감소시킨다. 뿐만 아니라, 비용이 들며 특정 사용자만을 쓰므로 비교 횟수도 줄어든다.

    
    
 ### 3. 비교성능에서 절대 성능으로 
 
 - 비용 대비 효과를 분석하기 어렵다.



---

4장에서는 3장의 평가 방법을 활용해 **애플리케이션의 모델**을 평가하는 방법에 대해 다룬다.

# 1) 평가 기준 

현재 투자 대비 효과를 확신할 수 없는 AI 애플리케이션이 꽤 흔하다.
이는 애플리케이션을 평가하기 어려울 뿐 아니라, 자신의 애플리케이션이 어떻게 사용되고 있는지 정확히 파악하지 못하기 때문이다. 

따라서, 먼저 애플리케이션을 **어떻게 평가할지 이해하는 것이 중요하다.**

### 평가 주도 개발

개발하기 전에 평가 기준을 정의하는 것

| **활용 분야** | **기준** |
|-------|--------|
| 추천 시스템 | 참여도, 구매 전활율 |
| 사기 탐지 시스템 | 예방한 사기로 돈을 얼마나 절약 |
| 코딩 | 기능적 정확성 |
| 파운데이션 모델 | 폐쇄형 작업에서 주로 평가 (분류, 다음 행동 예측) |

**단점**

결과 측정이 가능한 애플리케이션 개발에만 집중하는 것이다.

## 1-1. 도메인 특화 능력


모델이 애플리케이션이 필요로 하는 능력을 갖췄는지 평가하기 위해 도메인 특화 벤치마크를 활용할 수 있다.

ex) 코딩 - 정확성, 효율성, 코드 가독성 



## 1-2. 생성 능력

NLG (자연어 생성) 작업에서는 생성된 품질을 평가하는데 **유창성**과 **일관성**을 사용하였다.
번역 작업에 대해서는 **충실성**을 지표로 사용할 수 있고, 요약에서는 **관련성**을 평가할 수 있다.


현재는 **언어 모델의 생성 능력이 발전**하면서, 사람이 작성한 텍스트와 구분하는 것이 어려워졌다. **즉, 유창성과 일관성의 중요도가 낮아진 것이다.**


그러면 현재 생성 능력을 판단하기 위해 **더 중요하게 봐야하는 것은 무엇인가?**

### 사실 일관성

모델의 사실 일관성을 검증하기 위해서는 두가지 방식을 활용할 수 있다.

**1. 국소적 사실 일관성**

- 출력을 **context에 기반**해 평가한다. 따라서 실제 사실보다는 context에서의 사실이 무엇인지가 중요하다.

	
    - ex) 만약, context에 하늘은 보라색이다라는 내용이 있다면, 하늘은 파랗다는 사실과 일치하지 않는다고 본다.

**2. 전역적 사실 일관성**

- 출력을 **공개된 사실**에 기반해 평가한다.

- 따라서, 일반 챗봇, 사실 확인, 시장 조사 같은 **광범위한 작업**에서 중요하다.

**더 정교한 판단 기법으로 2가지가 있다.**

**1. 자체 검증**

> SelfCheckGPT가 가장 대표적 예시이다.
>
> - 모델이 서로 일치하지 않는 여러 출력을 생성하는 경우, 원래의 출력이 환각일 가능성이 높다는 가정을 따른다.
>
> - 평가할 응답이 주어지면, N개의 새로운 응답을 만들고, 이와 얼마나 일치하는지를 측정한다.
>
> 효과는 있지만, 응답을 평가하는데 많은 질의가 필요하므로 비용이 많이 든다.

**2. 지식 강화 검증**

> SAFE : 구글 딥마인드가 소개한 검색 증강 사실성 평가기이다.
> 이를 통해, 검색 엔진 결과를 활용해 응답을 검증한다.
>
> 4가지 절차를 거친다. 
>
>1. AI 모델을 사용해 응답을 개별 문장으로 분리한다.  
> 2. 각 문장이 독립적으로 이해될 수 있도록 수정한다. 예를 들어, “20세기에 개장했다”라는 문장에서 “개장했다”의 주어를 원래 주어로 바꾼다.  
> 3. 각 문장에 대해 구글 검색 API에 보낼 사실 확인 질의를 제안한다.  
> 4. AI를 사용해 문장이 검색 결과와 일치하는지 판단한다.

> 텍스트 함의(두 진술 간의 관계)를 파악해 출력한다.
>	- 함의: 가설은 전제로부터 추론할 수 있다. (**사실 일관성**)
>	- 모순: 가설은 전제와 모순된다. (**사실 비일관성**)
>	- 중립: 전제는 가설을 함의하지도, 모순되지도 않는다. 

<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/3866325f-2563-485a-bb8c-6a69f5367b16/image.png" />
<p align="center">검증 과정</p>


- 이러한 사실 일관성은 **검색 증강 생성(RAG) 시스템**의 중요한 평가 기준이다.

<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/2229fc2a-d84f-4e75-9b4d-a5d7651fcb4c/image.png" />
<p align="center">gpt-4 TruthfulQA에 대한 성능이 가장 좋은 것을 볼 수 있다.</p>


### 안정성

- 모델이 생성하는 결과물이 해로운지 아닌지를 판단한다. 

- 유해성을 측정하는 일반적인 벤치마크로는 `RealToxicityPrompts` `BOLD`가 있다. 

> 위험한 content는 다음과 같이 분류할 수 있다. 
> 1. 욕설과 노골적인 내용을 포함한 부적절한 언어  
? 2. 은행 강도 단계별 가이드나 자기 파괴적 행동을 하도록 사용자를 부추기는 것과 같은 유해한 추천과 지침  
>3. 인종 차별, 성차별, 동성애 혐오 발언 및 기타 차별적 행동을 포함한 혐오 발언  
>4. 위협과 자세한 묘사를 포함한 폭력  
> 5. 간호사는 항상 여성 이름을 사용하고 CEO는 남성 이름을 사용하는 것과 같은 고정관념  
> 6. 정치적 혹은 종교적 이데올로기에 대한 편향성은 해당 이데올로기를 지지하는 내용만 모델이 생성하게 할 수 있다.  

<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/610ba8a5-db27-4e18-aaa5-7b2159f12bfa/image.png" />
<p align="center"> 6번에 대한 예시로, 다양한 모델의 정치적, 경제적 성향 </p>

## 1.3 지시 수행 능력

- 이 모델이 주어진 지시를 얼마나 잘 따르는 지를 판단한다.

- 지시를 따르는 능력은 파운데이션 모델의 핵심 요구사항이며, 대부분 이를 위해 학습되는 것이다. 

	- 특히, JSON 형식이나 re 같은 구조화된 출력이 필요한 application에서 필수적이다. 

- InstructGPT는 지시를 따르도록 파인튜닝되었기 때문에 이런 이름이 붙었다. 

- 지시를 따르는 모델의 능력을 측정하는 벤치마크는 `IFEval`과 `INFOBench`가 있다. 


<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/008516de-056c-4ae2-b095-1d36bbaba93d/image.png" />
<p align="center"> 모델의 지시 준수 능력을 평가하는데 사용되는 그룹</p>


### 지시 유형

**1. 역할 연기**

- 모델에게 가상 캐릭터나 페리소나를 가정하도록 요청하는 것이다.
- 사용자가 상호 작용할 수 있도록 캐릭터를 연기하는 것, 프롬프트 엔지니어링 기법으로 역할 연기를 활용하는 방법이 있다. 

![](https://velog.velcdn.com/images/algorithm_cell/post/1c861367-d159-4f3c-a0e3-0546cb63cb53/image.png)


## 1-4. 비용과 지연 시간

이 내용은 9장에서 자세하게 다룰 예정이다. 

### 시간 지연

간단히만 살펴보면, **첫 토큰까지 걸리는 시간, 토큰 당 시간, 토큰 간 시간, 질의당 시간** 등 여러가지가 있다. 

지연 시간을 기준으로 모델을 평가할 때는 반드시 필요한 것과 있으면 좋은 것을 구분하는 것이 중요하다. 
사용자는 당연히 더 낮은 지연 시간을 선호하지만, 긴 지연 시간이 사용을 포기할 정도로 심각한 문제는 아니다.


### 비용

모델 API 사용하면 보통 토큰 단위로 요금을 부과하기 때문에, 입력과 출력 토큰을 많이 사용할 수록 비용이 더 많이 든다. 

![](https://velog.velcdn.com/images/algorithm_cell/post/70f6e97e-35a0-4a3f-a9cb-4fdf272b4ad6/image.png)

---

# 2) 모델 선택

## 2-1. 모델 선택 과정

**모델의 속성**

1. 하드 속성 : 변경이 불가능하거나, 비현실적인 속성
	
	- 일반적으로 모델 제공업체의 결정, 자체 정책의 결과인 경우가 많다.

2. 소프트 속성 : 변경할 수 있고, 기꺼이 변경할 의향이 있는 속성

	- 정확도, 유해성, 사실 일관성과 같이 개선할 수 있는 속성을 의민한다. 
    
	- 모델 활용 사례에 따라 정의되는 하드 속성과 소프트 속성은 다르다.
    
    	
        - ex) 만약 모델을 최적화해서 빠르게 실행한다면 **지연 시간**은 **소프트 속성**이 되지만, 이 호트싱하는 모델을 사용한다면 **하드 속성**이 된다. 
    
**평가 과정**
> 1. 하드 속성이 적합하지 않은 모델을 걸러낸다. 
하드 속성 목록은 자체 내부 정책과 상용 API를 사용할지 자체 모델을 호스팅할 지에 따라 크게 달라진다.  
>
>2. 공개된 정보(예: 벤치마크 성능과 리더보드 순위)를 활용해 **실험해 볼 가장 유망한 모델을 추려내되**, 모델 품질, 지연 시간, 비용 등 여러 목표를 균형 있게 고려한다. 
> 
> 3. 자체 평가 파이프라인으로 실험을 수행해 최적의 모델을 찾되, 마찬가지로 모든 목표를 균형 있게 고려한다.
> 
> 4. 운영 환경에서 모델을 지속적으로 모니터링하여 실패를 감지하고 애플리케이션 개선을 위한 피드백을 수집한다.

![](https://velog.velcdn.com/images/algorithm_cell/post/5737f7aa-d1b5-4633-89a2-65a189f0d687/image.png)

- 이 4단계는 **순환적이다**. 

## 2-2. 모델 자체 개발 or 상용 모델 구매

둘 중 선택을 할 때 고려할 사항은 7가지가 있다. 

상용 모델을 구매한다면 대부분 **API** 형태로 제공받게 된다 .

### 데이터 프라이버시

- 엄격한 데이터 프라이버시 정책으로 조직 외부에 데이터를 전송할 수 없는 기업은 **외부 호스팅 모델 API를 사용할 수 없다**.

### 데이터 계보와 저작관

응용 프로그램의 특허 가능성은 혁신에 대한 사람의, 기여도가 특허를 받기에 충분한지에 달려있는데, **저작권이 있는 데이터**로 학습한모델을 써서 제품을 만들었다면 **지적재산권을 지킬 수 있을 지 불분명하다**

따라서, 이러한 기업들은 학습 데이터를 공개적으로 제공하는 **완전 개방형 모델**로 방향을 틀었다. 

### 성능

- api를 사용하지 않는다면, 오픈 소스 모델을 활용해서 개발해야한다. 하지만 오픈 소스의 성능은 상용 모델보다 조금 낮은 수준으로 기대된다. 
![](https://velog.velcdn.com/images/algorithm_cell/post/65e96449-b631-4404-b38c-af612e5cd3c6/image.png)


### 기능

특정 활용 사례에 맞게 모델을 작동시키려면 여러 기능이 필요하다. 

**모델에 필요한 기능**

- 확장성 : 추론 서비스가 원하는 지연 시간과 비용을 유지하면서 애플리케이션 트래픽을 감당하도록 하는 것

- 함수 호출

- 출력 구조: 구조화된 출력

- 출력 가드레일 : 응답이 인종차별적이거나 성차별적이지 않도록 하는 것

이런 기능들은 실제로 구현하기 어렵고 시간도 많이 걸리기 때문에 이를 바로 사용할 수 있는 **API**를 활용하기도 한다. 하지만, API가 제공하는 기능만 활용할 수 있기 때문에 이에 대해 잘 생각해 봐야한다.

### 비용

API는 사용량에 따라 요금을 부과한다. 따라서 사용량이 많은 기업은 API 비용으로 자원을 낭비하는 것보다 모델을 직접 호스팅하는 방안을 검토할 수 있다. 

### 제어, 접근성, 투명성 

<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/b2e6c865-835a-4e75-b633-6fc299af1904/image.png" />
<p align="center"> 기업들이 오픈소스에 관심을 보이는 이유</p>

- 기업들이 오픈소스에 관심을 보이는 이유는 **제어**와 **커스터마이징 가능성**이다.

하지만, 특정 상용 모델에 의존하면 제공자의 안전 가이드라인, 검열, 제한된 조정 가능성, 버전 변경의 불투명성 등으로 인해 통제권을 잃고 예측 불가능한 위험에 노출될 수 있다.

모델 제공 중단, 국가 규제, 서비스 종료 같은 외부 요인까지 겹치면 안정적인 운영이 어려워지므로, 장기적으로는 자체 모델 구축이나 오픈소스 활용을 고려해야 한다.


### 온디바이스  배포

디바이스 자체에서 직접 모델을 실행하고 싶다면 써드파티 API는 사용할 수 없다.


**정리해보면**

| 구분 | 모델 API 사용하기 | 모델 자체 호스팅 |
|------|----------------------|----------------------|
| **데이터** |  데이터를 모델 제공자에게 보내야 하며, 이로 인해 팀에서 **비밀 정보를 유출**할 수 있다. | 데이터를 외부로 보낼 필요가 없지만, 데이터 계보나 학습 데이터 저작권에 대한 투명성이 낮거나 검증이 어렵다. |
| **성능** | 성능이 좋은 모델은 아마도 비공개 소스일 것이다. | 최고의 오픈 소스 모델도 아마 상용 모델보다 는 조금 뒤처진다. |
| **기능** | • 확장 지원 가능성이 높고, 함수 호출과 구조화된 출력 지원이 용이하다. <br>• 로그프롬을 노출할 가능성이 적다. | • 함수 호출과 구조화된 출력에 대한 지원이 없거나 제한적이다.<br>• 로그프롬과 중간 결과에 접근할 수 있어 분석, 평가, 해석 기능에 유용하다. |
| **비용** | API 비용 | 모델을 최적화, 호스팅, 유지보수하는 데 필요한 인재, 시간, 엔지니어링 노력 |
| **파인튜닝** | 모델 제공자가 허용하는 경우에만 모델을 파인튜닝할 수 있다. | 모델을 파인튜닝하고, 양자화하고, 최적화할 수 있다. |
| **제어, 접근 및 투명성** | •요청 제한<br>• 모델 접근 권한을 잃을 위험<br>• 모델 변경과 버전 관리의 투명성 부족 | • 오픈 소스 모델의 변경사항을  쉽게 확인할 수 있다.<br>• 모델을 동결하여 접근을 유지할 수 있다. 하지만 모델 API를 구축하고 유지하는 책임이 있다. |
| **온디바이스 활용 사례** | 인터넷에 연결되지 않은 기기에서는 실행할 수 없다. | 기기에서 실행할 수 있지만, 실제로 실행하기 어려울 수도 있다. |


## 2-3. 공개 벤치마크 탐색하기

여러 벤치마크에서 모델을 평가하는데 도움이 되는 도구는 **평가 하네스**이다.


### 공개 리더보드

- 일부 벤치마크의 종합 성능을 기반으로 모델의 순위를 매긴다.

- 이런 리더보드는 유용하지만, 모든 모델을 다루지는 못한다.
    
    - 컴퓨팅 제약이나 비용 문제로 몇몇 벤치마크는 제외한다.
    
    - 공개 리더보드는 평가 범위를 넓게 가져가기 때문에 다양한 능력을 두루 평가할 수 있는 핵심 벤치마크만 골라 사용한다. 


따라서, 공개 리더보드에서 높은 순위를 기록했다고 해서, 애플리케이션에서도 잘 작동한다는 **보장**이 있는 것은 아니기 때문에 **개인화**가 필요하다. 

**공개 리더보드를 확인했다면** 이제 공개 벤치마크에서 **애플리케이션만의 리더보드**를 만들어야한다.

### 맞춤형 리더보드 만들기

- 신뢰할 수 있는 벤치마크인지, 내가 만들고자 하는 애플리케이션에 적합한 부분의 성능이 높은지를 판단해서 선택해야한다. 

- 공개 벤치마크로 모델을 평가할 떄는, 더 엄격한 모델을 추려내는 과정이라는 것을 알고 있어야한다. 왜냐하면, **벤치마크가 애플리케이션의 필요를 완벽히 반영하지 못할 수도 있고, 오염되었을 가능성도 있다.**

#### 공개 벤치마크의 데이터 오염

대부분의 데이터 오염은 의도치 않게 발생한다. 오늘날 많은 모델은 인터넷에서 수집한 데이터로 학습되는데, 이 과정에서 공개된 벤치마크의 데이터가 포함됐을 수 있다.

그러면서 평가 벤치마크의 **신뢰성이 하락하고 있다.**

데이터 오염에 **대처하려면** 오염을 감지하고 데이터를 정화해야한다.

- **n-gram 중복** 체크

- **퍼플렉시티** 체크 : 값이 너무 낮다면 모델이 학습 과정에서 해당 데이터를 이미 접할 것일 수 있다. 


![](https://velog.velcdn.com/images/algorithm_cell/post/9b899763-4c57-49eb-926f-0d7d84755c2f/image.png)

- 오픈 AI는 GPT-3의 일반적인 벤치마크와의 오염도를 분석하면서, 학습 데이터에 40% 이상 포함된 벤ㄴ치마크가 13개나 된다는 것을 발견하였고, 오염되지 않은 샘플로만 평가했을 떄와의 성능 차이를 실험했다. 

---

# 3) 평가 파이프라인 설계

## 1단계. 시스템 구성요소 평가하기

## 2단계. 평가 가이드라인 만들기

- 명확한 **평가 가이드라인**을 만드는 것은 가장 중요한 작업이다.

- 애플리케이션이 해야 할 일뿐 아니라, **하면 안 되는 일**에 대해서도 정의해야한다.

**1. 평가 기준 정의하기**

정확한 답변이 항상 좋은 응답은 아니므로 어떤 지표가 애플리케이션을 평가하는데 중요한 지표인지 고려해야한다.


> ex) **고객지원 애플리케이션의 경우**
>- 관련성 : 응답이 사용자의 질의와 관련이 있다.
>
>- 사실 일관성 : 응답이 context와 사실적으로 일치한다.
>
>- 안전성 : 응답이 유해하지 않다.
>
>3가지 기준이 좋은 응답의 기준이 될 수 있다. 

**2. 예시와 함께 평가 기준표 만들기**

**3. 평가 지표를 비즈니스 지표와 연결하기**


## 3단계. 평가 방법과 데이터 정의하기

애플레리케이션을 평가하는데 사용할 방법과 데이터를 정의한다.

**1. 평가 방법 선택하기**

**2. 데이터 슬라이싱**

슬리이싱이란 데이터를 하위 집합으로 나누고, 각 하위 집합에 대한 시스템 성능을 개별적으로 살펴보는 것을 의미한다.

이 과정은 **편향 축소, 디버깅, 애플리케이션 개선 영역 발굴, 심슨의 역설 회피**에 도움이 될 수 있다.
 
****심슨의 역설*** : 모델 A가 집계된 데이터에서는 모델 B에 비해 성능이 좋지만, 데이터의 다른 모든 하위 집합에서는 성능이 떨어지는 현상 

<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/b6369d05-2001-4f4a-9b5a-232dde5bea91/image.png" />
<p align="center"> 심슨의 역설 예시 </p>


따라서 좋은 평가를 위해서는 다양한 데이터 슬라이스를 나타내는 여러 평가 세트를 갖춰야한다. 

중요하게 생각하는 부분에 대한 테스트 세트를 만들자. 이것을 **평가를 위해 선별되고 주석이 달린 데이터**라고 한다. 

 
**3. 평가 파이프라인 평가하기**

평가 파이프라인의 품질을 평가하기 위해 고려해야할 사항들이다.

- 평가 파이프라인이 올바른 신호를 제공하고 있는가?

- 평가 파이프라인을 얼마나 신뢰할 수 있는가?

- 지표 간 상관관계는 어떠한가?

- 평가 파이프라인이 애플리케이션에 얼마나 많은 비용과 지연 시간을 추가하는가?

**4. 반복**

평가 파이프라인을 **반복적으로 개선하면서 적절한 실험 추적을 수행**해야하고, 평가 데이터, 기준표, AI 평가자에 사용된 프롬프트 및 샘플링 구성을 포함하여 **평가 프로세스에서 변경될 수 있는 모든 변수를 기록한다**. 
