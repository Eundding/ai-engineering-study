# Week 06 - AI Engineering 인사이트

각 팀원이 작성한 study-note를 바탕으로 개인별 인사이트와 학습 내용을 정리합니다.

---

## 김성언 (Kim Seong-Eon)

### 전체적인 작성 내용 요약
7장 파인튜닝의 전반적인 내용을 가장 상세하게 다룸. 전이학습 개념부터 시작해 파인튜닝의 다양한 유형(지속적 사전학습, 인필링, 지도, 선호도, 롱컨텍스트), 파인튜닝 vs RAG 선택 기준, 메모리 병목 현상과 계산 방법, 수치 표현 방식(FP32/FP16/BF16/INT8 등), 양자화(PTQ/QAT), PEFT 기법들(어댑터, 소프트 프롬프트), LoRA의 작동 원리와 수학적 배경, QLoRA, 모델 병합 방법(합산/레이어 쌓기/연결), 파인튜닝 하이퍼파라미터까지 포괄적으로 정리. 특히 수식과 계산 예시가 풍부하고, GPT-3-175B 파인튜닝 시 학습 파라미터 수 계산 등 구체적인 수치 예시가 돋보임.

### 핵심 인사이트
- **"파인튜닝은 형식을 위한 것, RAG는 사실을 위한 것"**: 모델 오류의 원인이 정보 부족이면 RAG, 행동 방식 문제면 파인튜닝으로 명확한 선택 기준 제시
- **LoRA의 내재적 차원 개념**: LLM이 많은 파라미터를 가지고 있어도 실제로는 낮은 내재적 차원을 가지며, 사전학습이 일종의 압축 프레임워크 역할을 한다는 점
- **메모리 병목 해결의 핵심**: 학습 가능한 파라미터 수 감소(PEFT)와 양자화가 메모리 효율화의 두 축
- **LoRA 랭크 선택**: r=4~64 정도의 작은 값으로도 대부분의 활용 사례에서 충분

### 독특한 관점 또는 흥미로운 발견
- OpenAI의 파인튜닝 모범 사례를 "진행 경로"와 "증류 경로" 두 가지로 구분하여 실용적인 접근법 제시
- 모델 병합에서 "연결" 방식은 메모리 절약 효과가 없어 추천하지 않는다는 점 언급
- 프롬프트 캐싱 기술 도입 이후 파인튜닝의 토큰 최적화 장점이 줄었지만, 컨텍스트 길이 제한에서는 여전히 우위

### 실무 적용 아이디어
- 멀티 LoRA 서빙: 고객별 LoRA 어댑터를 별도로 유지하면 저장 공간 절약 가능 (예: W' 100개 vs W 1개 + A,B 100세트)
- 파인튜닝 전 RAG부터 시도하고, BM25 같은 단순 키워드 검색부터 시작할 것
- 학습률은 사전학습 마지막 학습률의 0.1~1배로 시작 권장

### 추가 학습이 필요한 부분
- 선형 결합과 구면 선형 보간법(SLERP)에 대해 "내용 보충 필요" 표시 → 모델 병합 수학적 원리에 대한 추가 학습 필요
- 모델 업스케일링과 뎁스와이즈 스케일링의 세부 동작 원리

### 오개념 정리
| 작성 내용 | 수정/보완 내용 | 비고 |
|----------|---------------|------|
| 특별한 오개념 없음 | - | 수식과 개념이 정확하게 정리됨 |

---

## 안태현 (Ahn Tae-Hyun)

### 전체적인 작성 내용 요약
HTML 형식으로 작성된 상세한 노트. 파인튜닝 개요(전이학습, 지속적 사전학습, 인필링, 지도, 선호도, 롱컨텍스트 파인튜닝), 파인튜닝 필요 여부 판단, 파인튜닝 vs RAG, 메모리 병목 현상(역전파, 메모리 계산, 수치 표현, 양자화), PEFT 기법(어댑터 기반, 소프트 프롬프트 기반), LoRA 상세 설명(구성, 서빙, 단점), QLoRA, 모델 병합(합산, SLERP, 가지치기, 레이어 쌓기, 연결), 파인튜닝 전술(프레임워크, 기본 모델, 하이퍼파라미터)까지 체계적으로 정리. GitHub 이미지와 수식을 적극 활용하여 시각적 이해를 돕고 있음.

### 핵심 인사이트
- **PEFT 기법 분류의 명확화**: 어댑터 기반(LoRA, BitFit, IA3, LongLoRA)과 소프트 프롬프트 기반(prefix-tuning, P-tuning, prompt tuning)으로 체계적 분류
- **LoRA 구성의 핵심**: 어텐션 행렬 4개(Wq, Wk, Wv, Wo) 중 두 개만 선택해야 한다면 Query와 Value 행렬이 가장 효과적
- **학습 메모리 공식**: 학습 메모리 = 모델 가중치 + 활성화 + 그래디언트 + 옵티마이저 스테이트
- **양자화 대상 우선순위**: 가중치 양자화가 활성화 양자화보다 더 안정적이고 정확도 손실이 적음

### 독특한 관점 또는 흥미로운 발견
- GitHub huggingface/peft 저장소의 1000개 이상 공개 이슈 분석을 통해 LoRA의 압도적 인기 시각화
- 투게더 AI의 "에이전트 혼합(mixture-of-agents)" 사례: 오픈소스 모델 6개 조합으로 GPT-4o에 필적하는 성능 달성
- 작업 벡터 개념을 통한 작업 산술(task arithmetic) 가능성: 벡터 더하기로 능력 결합, 빼기로 특정 기능 제거
- 프롬프트 손실 가중치의 기본값 10%: 응답 위주 학습하되 프롬프트 정보도 일부 반영

### 실무 적용 아이디어
- **그래디언트 누적**: 배치 크기 제약 시 여러 배치의 그래디언트를 모아 업데이트하여 메모리 효율화
- **LoRA 어댑터 공유**: 공개된 파인튜닝된 LoRA 어댑터를 사전학습 모델처럼 재사용 가능
- **불필요한 파라미터 가지치기**: 모델 병합 전 작업 벡터의 불필요한 파라미터를 0으로 재설정하여 간섭 방지
- **LoRA 서빙 전략**: 단일 모델은 미리 병합, 멀티 모델은 분리 유지

### 추가 학습이 필요한 부분
- 혼합 정밀도 학습에서 자동 혼합 정밀도(AMP) 기능의 실제 적용 방법
- 구면 선형 보간법(SLERP)의 수학적 원리와 보간 인자 선택 기준
- 저랭크 사전학습의 가능성과 한계(ReLoRA, GaLore 연구)

### 오개념 정리
| 작성 내용 | 수정/보완 내용 | 비고 |
|----------|---------------|------|
| 특별한 오개념 없음 | - | 책 내용을 정확히 반영하며 체계적으로 정리됨 |

---

## 이은정 (Lee Eun-Jeong)

### 전체적인 작성 내용 요약
깔끔한 마크다운 구조로 작성된 노트. 파인튜닝 개념과 5가지 유형(지속적 사전학습, 인필링, 지도, 선호도, 롱컨텍스트)을 체계적으로 정리. 파인튜닝 필요 여부 판단 기준, RAG와 파인튜닝 선택 기준, 메모리 병목 현상(역전파, 메모리 계산, 수치 표현, 양자화), PEFT 기법(어댑터 기반, 소프트 프롬프트 기반), LoRA 원리와 효과, 양자화된 LoRA(QLoRA), 모델 병합 방법들, 파인튜닝 전술까지 다룸. velog 이미지를 활용하여 시각적 이해를 돕고, 핵심 포인트를 간결하게 정리.

### 핵심 인사이트
- **파인튜닝 5가지 유형의 명확한 구분**: 지속적 사전학습(도메인 적응), 인필링(편집/보완 능력), 지도(입출력 쌍 학습), 선호도(강화학습 기반), 롱컨텍스트(위치 임베딩 조정)
- **메모리 계산 핵심 공식**: 추론 메모리 = N × M × 1.2, 학습 메모리 = 모델 가중치 + 활성화 + 그래디언트 + 옵티마이저 스테이트
- **옵티마이저별 스테이트 수**: SGD(0개), Momentum(1개), Adam(2개) - Adam 사용 시 메모리 증가 원인
- **LoRA 수식**: W' = W + (α/r)W_AB, α:r 비율은 1:8 ~ 8:1 사이에서 실험

### 독특한 관점 또는 흥미로운 발견
- **TF32의 비밀**: 32비트 레지스터를 사용하지만 실제 계산에는 19비트만 활용, 남은 비트는 의미 없음
- **BF16 vs FP16**: 같은 16비트지만 BF16이 더 넓은 범위(큰 값 표현 가능), 대신 정밀도는 떨어짐
- **프롬프트 캐싱 이후의 변화**: 파인튜닝의 토큰 최적화 장점 일부 완화, 하지만 컨텍스트 길이 제한은 여전히 파인튜닝의 강점
- **권장 개발 흐름**: 프롬프트 엔지니어링 → 예시 추가 → RAG(BM25부터) → 고급 RAG → 파인튜닝 순서

### 실무 적용 아이디어
- **활성화 메모리 절감**: 그래디언트 체크포인팅(활성화 재계산) 사용 - 메모리 절약하나 학습 시간 증가 트레이드오프
- **양자화 시점 결정**: PTQ(학습 후 양자화)가 모델을 직접 학습하지 않는 개발자에게 적합
- **단계별 정밀도 전략**: 사전학습은 고정밀, 파인튜닝은 저정밀로 접근성과 효율성 확보
- **프롬프트 손실 가중치**: 약 10%로 설정하여 응답 품질 향상에 집중하되 프롬프트 정보도 일부 반영

### 추가 학습이 필요한 부분
- LLM-QAT(가중치/활성화 4비트, 임베딩 16비트) 등 혼합 정밀도 학습의 구체적 구현 방법
- 자동 혼합 정밀도(AMP) 기능 활용법
- 모델 업스케일링과 뎁스와이즈 스케일링 기법

### 오개념 정리
| 작성 내용 | 수정/보완 내용 | 비고 |
|----------|---------------|------|
| 특별한 오개념 없음 | - | 핵심 개념과 공식이 정확히 정리됨 |

---

## 허채연 (Heo Chae-Yeon)

### 전체적인 작성 내용 요약
체계적인 구조와 실용적 관점으로 작성된 노트. 전이학습(파인튜닝, 특성 기반 전이)부터 파인튜닝 효과 및 한계, RAG vs 파인튜닝 선택 기준, 메모리 병목(메모리 계산, 수치 표현, 양자화), PEFT 기법(소프트 프롬프트, 어댑터), LoRA 상세 설명(효과적인 이유, 구성, 서빙, 단점), QLoRA(NF4, 페이징 최적화), 모델 병합(합산, 레이어 쌓기, 연결), 파인튜닝 전술(기본 모델, 방법, 프레임워크, 하이퍼파라미터)까지 종합적으로 다룸. velog 이미지 활용과 간결한 요약이 특징.

### 핵심 인사이트
- **전이학습의 두 가지 방법 구분**: 파인튜닝(가중치 업데이트)과 특성 기반 전이(임베딩 벡터 추출 후 다른 모델 활용, CV 분야에서 많이 사용)
- **LoRA 효과의 근본 원인**: LLM이 낮은 내재적 차원(intrinsic dimension)을 가지며, 사전학습이 저차원 내재 공간에 압축된 표현을 학습하기 때문
- **QLoRA의 핵심 기법**: NF4(정규분포 기반 4비트 양자화) + 페이징 최적화(GPU 메모리 부족 시 CPU-GPU 자동 전송)
- **파인튜닝 방법별 데이터 요구량**: 전체 파인튜닝은 최소 수천 개, PEFT는 수백 개로도 가능

### 독특한 관점 또는 흥미로운 발견
- **특성 기반 전이와 파인튜닝의 차이점 언급**: CV(컴퓨터 비전) 분야에서 특성 기반 전이가 많이 사용된다는 점 - 다른 노트에서는 언급되지 않은 관점
- **모델 병합의 온디바이스 활용**: 개인정보 보호, 네트워크 제약, 비용 절감을 위한 온디바이스 배포에서 모델 병합이 효과적
- **순차 파인튜닝의 재앙적 망각 문제**: 모델 병합이 이 문제를 해결하는 대안
- **LoRA 서빙과 저장의 트레이드오프**: W 재사용으로 저장 공간 절약 vs 병합 연산으로 지연 시간 증가

### 실무 적용 아이디어
- **파인튜닝 시작 전략**: LoRA 같은 PEFT로 시작 → 전체 파인튜닝 순서로 진행 권장
- **서빙 전략 결정**: LoRA는 전체 모델 하나 + 여러 어댑터, 전체 파인튜닝은 작업별 전체 모델 필요
- **MoE 구축**: 사전학습 모델의 레이어/모듈 복사 → 라우터 추가 → 추가 학습으로 성능 개선
- **프롬프트 손실 가중치 설정**: 기본값 10% - 프롬프트와 응답 학습 비중 조절

### 추가 학습이 필요한 부분
- 특성 기반 전이의 LLM 적용 가능성과 한계
- MoE(전문가 혼합) 모델의 라우터 학습 방법
- 모델 업스케일링(뎁스와이즈 스케일링 등)의 구체적 구현

### 오개념 정리
| 작성 내용 | 수정/보완 내용 | 비고 |
|----------|---------------|------|
| 특별한 오개념 없음 | - | 개념이 정확하고 실용적 관점에서 잘 정리됨 |

---

## 통합 토론 주제

스터디 세션에서 함께 논의하면 좋을 질문들을 자유롭게 추가해주세요.

### 공통 질문
- 실무에서 파인튜닝과 RAG 중 어떤 것을 먼저 시도해야 할까? "정보 부족 → RAG, 행동 문제 → 파인튜닝" 기준이 항상 명확한가?
- LoRA의 랭크(r) 값을 어떻게 결정해야 할까? 4~64 범위 내에서 최적값을 찾는 실험적 접근법은?
- 프롬프트 캐싱 기술이 도입된 현재, 파인튜닝의 비용 대비 효과는 어느 정도인가?
- 양자화(4비트, 8비트)가 모델 성능에 미치는 실질적 영향은? 어느 수준까지 양자화해도 괜찮은가?

### 심화 토론
- LoRA의 내재적 차원(intrinsic dimension) 개념: 사전학습이 압축 프레임워크 역할을 한다는 것의 의미와 시사점
- 모델 병합에서 "작업 산술(task arithmetic)"의 가능성: 벡터 덧셈/뺄셈으로 모델 능력을 조합/제거하는 것이 실제로 효과적인가?
- QLoRA의 NF4 양자화가 정규분포 가정에 기반한다면, 가중치 분포가 정규분포가 아닌 경우에는 어떤 문제가 발생할까?
- 전체 파인튜닝 vs PEFT: 데이터 양이 충분할 때 전체 파인튜닝이 항상 더 나은 성능을 보장하는가?

### 다음 주차 연결 포인트
- **8장 데이터셋 엔지니어링**: 파인튜닝에 사용되는 고품질 데이터셋 구축 방법, 데이터 증강, 합성 데이터 생성
- **9장 추론 최적화**: 양자화된 모델의 추론 최적화, 서빙 효율화 기법, 배치 처리 전략
- 파인튜닝된 모델의 평가 방법론과 벤치마킹 전략 (3-4장 평가 내용과 연계)
