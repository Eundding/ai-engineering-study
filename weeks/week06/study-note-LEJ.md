# Chap 7. 파인튜닝

# 파인튜닝
- **파인튜닝**은 모델 전체나 일부를 추가로 학습시켜 특정 작업에 맞게 모델을 조정하는 과정이다.
- 프롬프트 기반 방법과 비교하면 파인튜닝은 훨씬 **더 많은 메모리**를 필요로 한다.
- 파인튜닝 기법의 핵심 목표: **메모리 요구량 감소** 
- 메모리 효율성을 높인 **파라미터 효율적 파인튜닝(PEFT)**이 대세
- 파인튜닝은 **전이 학습**의 한 방법
   - 딥러닝 초기부터 전이 학습은 **학습 데이터가 부족하거나 구하기 어려운 작업에 좋은 해결책**이 되어왔다.
   - **LLM의 경우**, 텍스트 완성(데이터가 풍부한 작업)에서 사전 학습으로 얻은 지식은 법률 질의 응답이나 Text-to-SQL 변환 같은 더 전문적인 작업(데이터가 적은)으로 전이한다.
   - 전이 학습은 **표본 효율성**을 높여 모델이 더 적은 예시로도 같은 행동을 학습할 수 있게 한다.
    ex) 법률 질의 응답을 위해 처음부터 모델을 학습하려면 수백만 개의 예시가 필요할 수 있지만, 좋은 기본 모델을 파인튜닝하면 단 몇 백 개 만으로도 충분할 수도 있다.
> OpenAI의 InstructGPT 관련 논문(2022)에서는 파인튜닝을 모델이 이미 갖고 있지만 사용자가 프롬프트 만으로는 끌어내기 어려운 능력을 활용 가능하게 만드는 것으로 볼 수 있다고 말했다.


파인튜닝은 목적에 따라 여러 방식이 존재한다.

#### 지속적 사전 학습 (Continued Pre-training)
- 비싼 작업별 주석 데이터를 사용하기 전에, 저렴하고 대량으로 확보 가능한 관련 도메인 데이터로 먼저 학습시키는 방식이다.
- ex) 법률 Q&A 모델 → 질의·응답 데이터 이전에 법률 문서 전체로 학습
베트남어 요약 모델 → 요약 데이터 이전에 베트남어 텍스트 전반으로 학습
- 이 방식은 자기 지도 학습 기반이며, 모델이 도메인 언어·지식에 먼저 익숙해지도록 만든다는 장점이 있다.

#### 인필링 파인튜닝 (Infilling Fine-tuning)
- 언어 모델은 크게 **자기회귀 모델**(이전 토큰을 기반으로 다음 토큰 예측), **마스크 언어 모델**(문장 전체 문맥으로 빈칸 예측)으로 나뉜다.
- 이 개념을 파인튜닝에 적용한 것이 인필링 파인튜닝이다.
- 문장의 중간을 채우는 방식으로 학습하고 텍스트 편집, 코드 수정, 디버깅 작업에 특히 유용하다.
- 자기회귀 모델이라도 인필링 파인튜닝 가능
- **단순 생성보다 편집·보완 능력을 강화하는 방향의 파인튜닝**

#### 지도 파인튜닝 (Supervised Fine-tuning)
- 자기 지도 학습으로 얻은 방대한 지식은 사람의 의도나 선호와 바로 일치하지 않을 수 있는데, 이를 보정하는 것이 지도 파인튜닝이다.
- (입력, 출력) 쌍으로 모델 학습
   - 입력: 지시(instruction)
   - 출력: 응답(response)
- 응답 형태는 책 요약처럼 개방형, 분류 문제처럼 폐쇄형
- 단점
   - 고품질 지시 데이터는 비싸고 만들기 어렵다
   - 사실성, 도메인 전문성, 정확성이 요구됨

#### 선호도 파인튜닝 (Preference Fine-tuning)
- 모델을 사람이 더 좋아하는 응답을 생성하도록 만드는 단계이고 **강화학습** 기반이다.
- 주로 (지시, 선호 응답, 비선호 응답) 형태의 비교 데이터 사용
- 사용자 경험(UX)을 크게 좌우하는 핵심 단계
- 정답보다 **사람이 선호하는 답변**을 학습

#### 롱 컨텍스트 파인튜닝 (Long-context Fine-tuning)
- 모델이 더 긴 입력을 처리하도록 만드는 파인튜닝 방식
- 컨텍스트 길이 증가 = 더 많은 토큰 위치 필요
- 보통 위치 임베딩 수정 등 구조 변경이 필요
- 난이도가 높고, 짧은 문맥 성능이 오히려 떨어질 수 있다.
- 따라서 다른 파인튜닝 기법보다 신중한 적용이 필요

![](https://velog.velcdn.com/images/dkan9634/post/f8b6cecc-1902-4e85-8db6-94a4b075c7d9/image.png)

-> 다양한 파인튜닝 기법을 사용해 기본 모델 라마 2에서 여러 코드 라마 모델을 개발하는 과정

---
# 파인튜닝이 필요한 경우
파인튜닝은 많은 데이터, 고사양 하드웨어를 요구하기 때문에 보통은 **프롬프트 기반 방법을 충분히 시도한 후에 파인튜닝을 시도하는 것이 일반적**이다. 

## 파인튜닝을 해야 하는 이유
- 목적 : 일반 능력, 특정 작업 수행 능력을 모두 향상시키기 위해(특히 JSON이나 YAML 같은 특정 구조의 출력을 생성할 때, 파인튜닝이 효과적)
- 범용 모델이 여러 벤치마크에서 뛰어난 성능을 보이더라도,
특정 도메인이나 사용자 환경에서는 성능이 떨어질 수 있다.
   - 해결책 : **해당 작업과 직접적으로 관련된 데이터로 파인튜닝하는 것**
   - ex) 기본 모델이 표준 SQL 변환은 잘하지만 고객 맞춤형 SQL 쿼리에서 자주 실패한다면,그 고객 쿼리 데이터로 파인튜닝하는 것이 성능 개선에 큰 도움이 된다.

#### 편향 완화에도 효과적
- 파인튜닝은 모델의 편향을 줄이는 데에도 활용될 수 있다.
- 학습 데이터의 특성으로 인해 특정 성별·인종·직업에 대한 편향이 있다면, 신중하게 선별된 데이터로 파인튜닝하여 이를 완화할 수 있다.
   - 실제로 여성 작가 텍스트나 아프리카 작가 텍스트로 파인튜닝한 경우, 성별·인종 편향이 감소했다는 연구 결과도 보고되었다.

#### 작은 모델의 강력한 활용
- 대형 모델을 그대로 사용하는 것보다, 작은 모델을 파인튜닝하는 방식이 더 실용적인 경우가 많다.
- 메모리 요구량이 적고 비용이 저렴하며, 실제 서비스 환경에서 빠르게 동작한다.
- 또한 큰 모델이 생성한 데이터를 이용해 **작은 모델을 학습시키는 증류(distillation) 방식**도 널리 사용된다.
   - 흥미롭게도, 특정 작업에 파인튜닝된 작은 모델이 같은 작업에서 더 큰 기본 모델보다 뛰어난 성능을 보이는 사례도 존재한다.

## 파인튜닝을 하지 말아야 하는 이유
- 파인튜닝이 모델을 여러 면에서 개선할 수는 있지만, 이런 개선 대부분은 파인튜닝 없이도 어느 정도 달성할 수 있다.
   - 잘 작성된 프롬프트와 컨텍스트도 비슷한 효과를 낸다.

#### 특정 작업에만 강해질 위험
- 특정 작업을 위해 모델을 파인튜닝하면 그 작업에서는 성능이 향상될 수 있지만, 다른 작업에서는 **오히려 성능이 떨어질 수 있다.**
![](https://velog.velcdn.com/images/dkan9634/post/639b7721-9c6e-460a-9088-b926f9608e09/image.png)

-> 오히려 범용 모델이 도메인 특화 모델모다 성능이 더 높은 사례도 있다.

#### 운영·비용 측면의 부담
- 프로젝트 초기 단계라면, 파인튜닝은 가장 먼저 시도할 방법이 아니다.
- 고품질 주석 데이터 수집에 시간과 비용이 많이 든다
- 지속적인 관리와 재학습이 필요하다
- 비판적 사고나 도메인 전문 지식이 필요한 작업일수록 부담이 커진다
- 오픈소스 데이터나 AI 생성 데이터로 비용을 줄일 수는 있지만,
그 효과는 상황에 따라 크게 달라진다.

#### 대안
- 프롬프트 엔지니어링
- 컨텍스트 확장
- 작업별 서로 다른 모델 사용
- 필요하다면 모델 병합(ensemble)

이런 방법들이 파인튜닝보다 더 현실적인 선택일 수 있다.

#### 프롬프팅 vs 파인튜닝
- 프롬프팅과 파인튜닝 모두 체계적인 실험 접근이 필요하다.
- 프롬프트를 실험하는 과정에서 평가 파이프라인, 데이터 주석 가이드라인, 실험 추적 방법을 구축할 수 있으며, 이는 그대로 파인튜닝의 기반이 된다.
- **프롬프팅의 한계** : 프롬프팅은 예시를 많이 넣을수록 성능이 좋아질 수 있지만, 그만큼 **입력 토큰 수가 증가해 추론 지연이 늘어나고 비용이 증가**한다.
   - 매번 프롬프트에 예시를 포함하는 방식은 장기적으로 비효율적이다.
- **파인튜닝의 장점** : 반복적으로 사용하는 예시를 모델 내부로 흡수하면, 더 짧은 프롬프트로도 동일한 성능을 낼 수 있다.
   - 즉, 프롬프트에 포함하던 패턴을 **파인튜닝으로 일반화**하는 것이다.
- **프롬프트 캐싱 이후의 변화**
   - 프롬프트 캐싱 기술이 도입되면서 프롬프팅의 비용 문제는 일부 완화되었다. 
   - 하지만 여전히 프롬프팅은 컨텍스트 길이 제한을 받는 반면 파인튜닝은 활용 가능한 예시 수에 제한이 없다는 차이가 있다.
   ![](https://velog.velcdn.com/images/dkan9634/post/524d861c-af39-405f-bf00-e5c5dac7318b/image.png)

## 파인튜닝과 RAG
프롬프팅으로 모델 성능을 최대한 끌어올린 뒤에는 **RAG를 적용할지, 파인튜닝을 할지**를 선택해야 한다. 이 선택의 기준은 간단하다.
모델 오류의 원인이 **정보 부족인지 행동 방식의 문제인지**이다.

#### 정보가 부족한 경우 → RAG가 효과적
- 모델이 오답을 내는 이유가 내부에 해당 정보를 아예 모르거나
알고 있는 정보가 오래되었기 때문이라면 외부 지식에 접근하게 해주는 RAG가 더 적합하다.
- 예를 들어, 조직 내부 문서, 최신 뉴스·통계, 비공개 데이터처럼 **모델이 학습할 수 없었던 정보가 필요한 경우에는 파인튜닝보다 RAG가 훨씬 효율적**이다.
- 실제로 최신 정보·시사 문제를 다룬 연구에서는 RAG가 파인튜닝보다 더 좋은 성능을 보인다는 결과도 보고되었다.

![](https://velog.velcdn.com/images/dkan9634/post/c7d610d9-84b3-4652-a0bc-ec9a10ee0f6b/image.png)

-> 기본 모델을 사용한 RAG가 파인튜닝된 모델을 사용한 RAG보다도 더 나은 결과를 나타냈다. 이는 **파인튜닝이 특정 작업의 성능은 높여줄 수 있지만, 다른 영역에서는 오히려 성능이 떨어질 수 있음을 보여준다.**

#### 행동 방식이 문제인 경우 → 파인튜닝이 효과적
- 출력이 요청과 무관하거나 원하는 형식(JSON, HTML 등)을 제대로 지키지 못하거나 특정 문서 스타일이나 규칙을 따르지 못한다면 이는 지식 문제가 아니라 **행동 문제**다.
- 이 경우에는 관련 예시를 학습시키는 파인튜닝 특히 **시맨틱 파싱(자연어 → 구조화 출력)** 작업에서 파인튜닝이 중요한 역할을 한다.

> 파인튜닝은 형식을 위한 것이고,
RAG는 사실을 위한 것이다.

**RAG**는 환각을 줄이는 데 유리하고, **파인튜닝**은 특정 문체·규칙·출력 구조를 안정화하는 데 유리하다. (단, **파인튜닝**은 데이터 품질이 낮으면 오히려 환각을 키울 수 있다.)

![](https://velog.velcdn.com/images/dkan9634/post/6323631c-00f9-4a13-bf30-f4546e52e3da/image.png)

-> **모든 애플리케이션에 딱 맞는 보편적인 개발 과정은 없다.**

그래서 권장되는 개발 흐름은 다음과 같다.

1. 프롬프트 엔지니어링으로 먼저 해결 시도
2. 예시를 충분히 추가(1~50개)
3. 정보 부족 문제가 보이면 RAG 도입
- 처음에는 BM25 같은 단순 검색부터
- 계속 정보 관련 오류가 발생하면 임베딩 기반 검색 같은 고급 RAG 방법
4. 행동·형식 문제가 남으면 파인튜닝 고려
- 모델이 관련 없는 내용 생성하거나 형식이 잘못되거나, 안전하지 않은 응답을 생성하는 등의 행동 측면의 문제
5. 필요하다면 RAG + 파인튜닝 병행

중요한 점은, 평가는 전 과정에서 지속적으로 이루어져야 한다는 것이다.

파인튜닝의 가장 큰 난관인 메모리 병목 현상을 알아보자.

---
# 메모리 병목 현상
파인튜닝은 모델의 성능을 높일 수 있지만, 막대한 메모리를 요구한다.
그래서 많은 파인튜닝 기법들은 성능 자체보다 메모리 사용량을 줄이는 것에 초점을 둔다.
이러한 메모리 병목 현상을 이해하면, 상황에 맞는 파인튜닝 방법을 선택하는 데 도움이 된다.

#### 왜 메모리 병목이 생길까?
- 모델 규모가 매우 크다
- 파운데이션 모델은 추론뿐 아니라 파인튜닝 시에도 메모리의 주요 병목 지점이 된다.
- 학습 시 메모리가 더 많이 필요하다.
- 파인튜닝은 추론과 달리 **가중치, 그라디언트, 옵티마이저 상태**를 모두 저장해야 하므로 메모리 사용량이 크게 증가한다.
- 학습 가능한 파라미터 수가 많을수록 메모리 사용량 증가
   - 이 때문에 학습 파라미터 수를 줄이는 방식이 중요해졌고,
이것이 **PEFT(Parameter-Efficient Fine-Tuning)**의 핵심 아이디어다.

#### 메모리를 줄이는 핵심 방법
- 학습 파라미터 수 감소
   - LoRA, Adapter 등 PEFT 기법 사용
- 낮은 정밀도 사용
   - FP32 → FP16 → INT8
   - 예를 들어, 130억 파라미터 모델은
FP32: 약 52GB
FP16: 약 26GB 메모리 필요
- 추론과 학습의 정밀도 차이
   - 추론: 16비트, 8비트, 심지어 4비트 가능
   - 학습: 너무 낮은 정밀도는 불안정 → 혼합 정밀도 사용
   
 ## 역전파와 학습 가능한 파라미터
 - 파인튜닝 과정에서 모델의 메모리 사용량을 결정하는 핵심 요소는 **학습 가능한 파라미터의 수**이다.
    - 파인튜닝 중에 **업데이트 될 수 있는 파라미터**
 ![](https://velog.velcdn.com/images/dkan9634/post/5ac48cb5-17ef-4c2d-ac14-049622a79c4b/image.png)
 
 -> 세 개의 파라미터와 하나의 비선형 활성화 함수를 가진 가상의 신경망에 대한 순방향 및 역방향 패스를 시각화한 것
 +) 역방향 패스 동안 각 학습 가능한 파라미터는 그래디언트와 옵티마이저 스테이트 같은 추가 값들을 동반 => 이렇게 추가 값을 저장하기 위해 메모리도 더 많이 필요
 
 ## 메모리 계산
 - 모델에 적절한 하드웨어를 선택하기 위해 모델에 필요한 메모리 양을 미리 아는 것은 유용하다.
 
 ### 추론에 필요한 메모리
 - 추론 중에는 **순방향 패스만** 실행된다.
    - 순방향 패스에는 모델 가중치를 위한 메모리가 필요하다.
    - N: 모델의 파라미터 수, M: 각 파라미터에 필요한 메모리일 때, **모델 파라미터를 로드하는 데 필요한 메모리는 N X M**
    - 순방향 패스에는 활성화 값을 위한 메모리도 필요하다.(많은 애플리케이션에서 활성화 및 key-value 벡터에 필요한 메모리는 모델 가중치 메모리의 약 20%로 가정) 모델의 메모리 사용량은 **N X M X 1.2**
    - ex) 130억 파라미터를 가진 모델, 각 파라미터에 2바이트가 필요하다면?
    	- 모델 가중치에는 **130억 X 2바이트 = 26GB**
        - 추론에 필요한 총 메모리는 **26GB X 1.2 = 31.2GB**
- 모델이 커질수록, 메모리는 모델 운영의 병목이 된다.
- 파라미터당 2바이트를 사용하는 700억 파라미터 모델은 가중치만으로도 무려 140GB의 메모리가 필요하다.

### 학습에 필요한 메모리
- 모델을 학습시키려면 **모델 가중치와 활성화를 위한 메모리가 필요**하다.
   - 여기에 추가로 **그래디언트와 옵티마이저 스테이트를 위한 메모리**도 필요한데, 이는 학습 가능한 파라미터 수에 비례
- 전체적으로 학습에 필요한 메모리는 다음과 같이 계산된다.
> **학습 메모리 = 모델 가중치 + 활성화 + 그래디언트 + 옵티마이저 스테이트**

- 역전파(backpropagation) 동안, 각 학습 가능한 파라미터마다 다음이 필요하다.
   - 그래디언트 1개, 옵티마이저 상태 0~2개 (옵티마이저에 따라 다름)
   - 옵티마이저별 차이는 다음과 같다.
     - SGD: 옵티마이저 스테이트 없음
     - Momentum: 파라미터당 1개
     - Adam: 파라미터당 2개 (1차·2차 모멘트)
   - 예를 들어 Adam으로 130억 파라미터 전체를 학습하면 각 파라미터당 3개의 값(그래디언트 + 2개 상태)을 저장해야 한다.
     - 130억 × 3 × 2바이트 ≈ 78GB
   - 반면, LoRA처럼 학습 파라미터를 10억 개로 줄이면 10억 × 3 × 2바이트 ≈ 6GB
 	=> 이것이 PEFT가 메모리를 획기적으로 줄이는 이유다.

- 이전 공식에서 활성화에 필요한 메모리가 모델 가중치 메모리보다 적다고 가정한 것이 중요하다.(실제론 활성화 메모리가 훨씬 더 클 수 있다.)
   - 그래디언트 계산을 위해 활성화를 저장한다면, 활성화 메모리는 모델 가중치 메모리를 훨씬 웃돌 수 있다.

![](https://velog.velcdn.com/images/dkan9634/post/266984c7-4aa7-471b-b695-beea5181ced4/image.png)

#### 해결책
1. 그래디언트 체크포인팅
2. 활성화 재계산 (Activation Recomputation)
=> 모든 활성화를 저장하지 않음, 역전파 시 필요한 활성화를 다시 계산
- 장점: **메모리 사용량 크게 감소**
- 단점: **재계산 때문에 학습 시간 증가**
=> 메모리 ↔ 시간의 트레이드오프

> 파인튜닝의 메모리 병목은 가중치보다 그래디언트·옵티마이저·활성화에서 발생한다.

+) 그래서 실무에서는 보통 다음을 함께 사용한다.
- PEFT(LoRA 등) → 학습 파라미터 수 감소
- 활성화 재계산 → activation 메모리 절약


### 수치 표현 방식
- 지금까지 메모리 계산에서 각 값이 2바이트의 메모리를 차지한다고 가정했다. **모델에서 각 값을 표현하는 데 필요한 메모리는 모델의 전체 메모리 사용량에 직접적인 영향을 미친다.**
   - 즉, 각 값에 필요한 메모리를 절반으로 줄이면, 모델 가중치에 필요한 메모리도 절반으로 줄어든다.
- 신경망의 수치 값은 전통적으로 **부동소수점 수**로 표현된다.
   - 가장 일반적인 부동소수점 형식은 전기전자기술자협회(IEEE)의 부동소수점 산술 표준(IEEE 754)을 준수하는 **FP 계열**이다.
      - FP16: 16비트(2바이트) 사용, half precision
      - FP32: 32비트(4바이트) 사용, single precision
      - FP64: 64비트(8바이트) 사용, double precision
- FP64는 메모리 사용량 때문에 신경망에선 거의 사용되지 않는다.
- 다른 인기 있는 부동소수점 형식은 구글이 TPU에서 AI 성능을 최적화하기 위해 설계한`BFloat16(BF15)`과 엔비디아가 GPU를 위해 설계한`TensorFloat-32(TF32)`가 있다.
- 정수 표현 방식은 아직 부동소수점 형식마늠 일반적이지 않지만, 점점 더 인기를 얻고 있다. ex) INT8, INT4
- 부동소수점 숫자는 보통 세 부분의 비트로 나뉜다.
1. **부호 비트 (1비트)**
→ 숫자가 양수인지 음수인지 표시
2. **범위 비트 (Exponent)**
→ 표현할 수 있는 값의 크기 범위를 결정
→ 비트가 많을수록 더 큰 수, 더 작은 수까지 표현 가능
→ 자릿수가 많을수록 큰 수를 표현할 수 있는 것과 유사
3. **정밀도 비트 (Mantissa / Precision)**
→ 숫자를 얼마나 정확하게 표현할 수 있는지를 결정
→ 비트 수가 줄면 소수점 이하 정보가 줄어들어 값이 근사됨

- 보통 **비트 수가 많은 형식일수록 정밀도가 높다고 간주**된다.
- 고정밀도 형식을 저정밀도 형식으로 변환(FP32 -> FP16)하면 정밀도가 낮아진다.
   - 정밀도를 낮추면 값이 변하거나 오류가 발생할 수 있다.
![](https://velog.velcdn.com/images/dkan9634/post/45c94ca1-e8a0-45ab-a9b4-061cb5ab370b/image.png)

Q. TF32는 32비트인데 왜 19비트밖에 사용 안할까?
A. FP32 레지스터(32비트)를 그대로 사용하지만 정밀도는 FP16 수준으로 줄여서 계산한다. 즉, **32비트를 저장하긴 하지만 실제 계산에 쓰는 정보는 일부뿐이다.** 남은 비트는 0으로 채워지거나 무시되거나 내부적으로 쓰이지 않는다. => 의미 없다.

+) BF16, FP16은 같은 비트 수를 가졌지만 BF16이 더 큰 값을 표현하고 정밀도가 떨어진다.

## 양자화
- **모델 값을 표현하는 데 필요한 비트 수가 적을수록, 모델의 메모리 사용량도 줄어든다.**
- **양자화**: 정밀도를 낮추는 것
- ML 분야에서 저정밀도는 표준 FP32보다 적은 비트를 가진 모든 형식을 의미한다.


#### 무엇을 양자화?
- 원칙적으로는 메모리를 가장 많이 차지하는 요소부터 양자화하는 것이 이상적이다.
- 추론 시 메모리에 가장 큰 영향을 미치는 것은 모델 가중치와 활성화 값이며, 이 중 가중치 양자화가 활성화 양자화보다 더 일반적으로 사용된다.
   - 이유: 가중치 양자화가 더 안정적, 정확도 손실이 상대적으로 적음
   
#### 언제 양자화?
- **학습 중 또는 학습 후**에 수행할 수 있다.
- 가장 널리 쓰이는 방식은 **학습 후 양자화(PTQ, Post-Training Quantization)**이다.
   - 이미 학습된 모델에 적용, 모델을 직접 학습하지 않는 애플리케이션 개발자에게 적합
   - PyTorch, TensorFlow, Hugging Face 등 주요 프레임워크에서 기본 지원
   
### 추론 양자화
- 과거에는 FP32가 표준이었지만, 현재는 16비트 이하 정밀도로 추론하는 것이 일반적이다.
   - INT8: LLM.int8 (LLM을 8비트로)
   - INT4: QLoRA (4비트로 양자화)
   - 혼합 정밀도: 상황에 따라 정밀도를 조절
- 최근에는 FP8, FP4 같은 미니플로트 형식, INT8 / INT4 같은 정수 기반 양자화
가 함께 연구, 활용되고 있다.

#### 어디까지 줄일 수 있을까?
이론적으로는 1비트 이하로는 불가능하다.
- 1비트 시도 사례: `BinaryConnect`, `XNOR-Net`, `BitNet`
   - 특히 `BitNet b1.58`은 파라미터당 1.58비트 최대 39억 파라미터까지
     → 16비트 LLaMA 2와 유사한 성능을 보였다.

#### 왜 양자화를 쓸까?
- 정밀도를 낮추면 **장점**
   - 메모리 사용량 감소
   - 더 큰 배치 사용 가능
   - 계산 속도 향상 → 추론 지연 감소

- **단점**
   - 작은 수치 오차가 누적될 수 있음
   - 표현 범위를 벗어나면 무한대(INF) 또는 큰 오차 발생
   - 형식 변환 비용 때문에 항상 속도가 빨라지는 것은 아님
   
#### 요즘 일반적인 흐름
- 학습은 높은 정밀도(FP32/FP16)로, 추론은 낮은 정밀도(INT8/INT4)로 수행한다.
- 특히 모바일·엣지 환경에서는 양자화된 추론만 지원하는 경우도 많아 `TensorFlow Lite`, `PyTorch Mobile` 등이 **PTQ**를 기본 제공한다.

### 학습 양자화
- PTQ만큼 보편적이진 않지만 점점 인기를 얻고 있다.
- 목표 2가지
#### 1. **추론 과정에서 낮은 정밀도에서도 우수한 성능을 보이는 모델을 만드는 것**. 
- 이는 학습 후 양자화 과정에서 모델 품질이 저하될 수 있는 문제를 해결하기 위한 것
- 이를 위해 사용되는 대표 기법이 QAT (Quantization-Aware Training)
   - QAT는 학습 중에 **저정밀도 연산을 시뮬레이션**하지만 파라미터 업데이트는 고정밀로 수행되므로 학습 시간은 줄어들지 않고 오히려 늘어날 수도 있다.
   
#### 2. **학습 시간과 비용을 줄이는 것**. 
- 양자화는 메모리 사용량을 줄여 더 저렴한 하드웨어에서 모데을 학습하거나 같은 하드웨어에서 더 큰 모델을 학습할 수 있게 한다. 또한 계산 속도를 높여 비용도 추가로 절감할 수 있다.
- 단, 역전파는 저정밀도에 매우 민감해 학습 난이도가 높다
- 최근 연구에서는 INT8 완전 학습, 학습/추론 정밀도 불일치 문제 해소, 학습 효율 대폭 개선 같은 성과도 보고되고 있다.

+) QAT 자체는 학습 시간을 줄이지는 않지만, **모델을 처음부터 낮은 정밀도로 학습**하는 접근은 학습 시간과 비용을 줄이는 데 도움이 될 수 있다.

실무에선 혼합 정밀도가 표준이다.
- 완전 저정밀 학습은 어렵기 때문에, 대부분은 **혼합 정밀도(mixed precision)**를 사용한다.
   - 민감한 값(가중치 사본 등)은 높은 정밀도
   - 그래디언트·활성화 등은 낮은 정밀도
   - 일부 파라미터만 선택적으로 고정밀 유지 가능
- ex) LLM-QAT: 가중치,활성화 → 4비트, 임베딩 → 16비트 유지
이러한 설정은 수동으로 정할 수도 있고 **AMP(Automatic Mixed Precision)** 기능으로 자동화 가능

#### 단계별로 정밀도를 다르게 쓰는 전략
- 사전학습: 높은 정밀도 (대규모 자원 필요)
- 파인튜닝: 낮은 정밀도 (접근성·효율 중시)
→ 실제로는 대형 조직은 고정밀 학습, 일반 개발자는 저정밀 파인튜닝을 수행하는 구조가 흔하다.

---
# 파인튜닝 기법

## 파라미터 효율적 파인튜닝
- 전체 파인튜닝에서는 학습 가능한 파라미터의 수 = 전체 파라미터 수
- 전체 파인튜닝은 학습과 비슷해 보일 수 있지만, 주요 차이점은 **학습은 무작위화된 모델 가중치로 시작하는 반면, 파인튜닝은 이미 학습된 모델 가중치에서 시작**한다.
- 전체 파인튜닝은 높은 메모리와 데이터 요구사항 때문에 **부분 파인튜닝**을 시작하게 됐다.
   - 부분 파인튜닝에선 모델 파라미터의 일부만 업데이트된다.
   - 메모리 사용량을 줄일 수 있지만 파라미터 효율성이 떨어진다.
   - 전체 파인튜닝에 근접한 성능을 내기 위해 여전히 **많은 학습 가능한 파라미터가 필요**하다.
   
![](https://velog.velcdn.com/images/dkan9634/post/0321efb8-2ba4-4068-ba00-00eb50345664/image.png)

-> 학습 가능한 파라미터 수에 따른 부분 파인튜닝의 성능 곡선

#### 어떻게 하면 훨씬 적은 파라미터를 사용하면서도 전체 파인튜닝에 가까운 성능을 달성할 수 있을까?
=> **파라미터 효율적 파인튜닝(PEFT, Parameter-Efficient Fine-Tuning)**
- 명확한 기준은 없지만 수십 배 더 적은 학습 가능한 파라미터를 사용해 전체 파인튜닝에 근접한 성능을 달성할 수 있다면 파라미터 효율적이라고 본다.

#### Adapter 기반 PEFT
- PEFT의 대표적 초기 접근은 어댑터(Adapter) 방식이다.
- 기존 모델 파라미터는 동결, BERT 모델의 각 트랜스포머 블록에 두 개의 어댑터 모듈을 삽입하고 학습 시에는 어댑터만 업데이트했다.
   - 성과: BERT 기준(전체 파라미터의 약 3%만 학습, 전체 파인튜닝 대비 성능 차이 0.4% 이내)
   
  ![](https://velog.velcdn.com/images/dkan9634/post/409a820a-e09c-4fe0-b58e-b3e69fb3672c/image.png)

- PEFT의 **장점**
   - 메모리 사용량 대폭 감소
   - 저렴한 하드웨어에서도 파인튜닝 가능해 훨씬 많은 개발자가 활용할 수 있음
   - 필요한 데이터 수가 적음 (샘플 효율적)
      - **전체 파인튜닝**이 눈에 띄는 품질 향상을 위해 수만에서 수백만 개의 예시가 필요한 반면, 일부 PEFT 방법은 단 몇 천 개의 예시만으로도 강력한 성능을 낼 수 있다.
- **단점**
   - 어댑터 등 추가 모듈로 인해 추론 시 계산 단계 증가
   - 추론 지연(latency) 증가

### PEFT 기법들
1. 어댑터 기반 방법
- **모델 가중치에 추가 모듈을 붙이는 모든 방식**
   - **파라미터를 추가**하기 때문에 부가적 방법
- ex) `LoRA`, `BitFit`, `IA3`, `LongLoRA`(LoRA의 변형)

2. 소프트 프롬프트 기반 방법
- 학습 가능한 토큰을 도입해 모델이 입력을 처리하는 방식을 바꾼다.
   - 추가 토큰들은 입력 토큰과 함께 모델에 주입된다.
- **하드 프롬프트**
   - 사람이 읽을 수 있다.
   - 고정되어 학습 불가능
- **소프트 프롬프트**
   - 임베딩 벡터와 비슷한 연속적인 벡터로 사람이 읽을 수 없다.
   - 튜닝 과정에서 역전파를 통해 최적화할 수 있어 특정 작업에 맞게 조정할 수 있다.

![](https://velog.velcdn.com/images/dkan9634/post/e455dbe0-9abd-49a6-bd89-be4cd47dbbb2/image.png)

-> 소프트, 하드 프롬프트를 사용해 모델 동작을 유도하는 방법을 보여줌

- **분야**
   - prefix-tuning: 모든 트랜스포머 레이어 앞에 소프트 프롬프트 삽입
   - P-tuning
   - prompt-tuning: 입력 임베딩 앞에만 삽입
   
![](https://velog.velcdn.com/images/dkan9634/post/a6666d07-9dcf-40cf-a308-b33bef15610f/image.png)

-> 2024년 10월 기준으로 깃허브의 `huggingface/peft` 저장소에 올라온 1000개 이상의 공개 이슈 분석

### LoRA
- 어댑터 방식과 달리, 추론 지연 시간을 추가로 발생시키지 않으면서 파라미터를 효과적으로 추가한다.
- 기본 모델에 새로운 레이어를 추가하는 대신, LoRA는 원래 레이어와 병합할 수 있는 모듈을 활용한다.
- 개별 가중치 행렬에 적용할 수 있다. 특정 가중치 행렬이 주어지면, **LoRA는 이 행렬을 두 개의 더 작은 행렬의 곱으로 분해하고, 이 작은 행렬들을 업데이트한 후 다시 원래 행렬로 병합하는 방식**
![](https://velog.velcdn.com/images/dkan9634/post/88689a24-4551-42d0-8c42-45f5800a5fc0/image.png)


![](https://velog.velcdn.com/images/dkan9634/post/3a130a98-7c9a-41ba-9c29-4b8e104a2e62/image.png)

- 기존 어댑터 방식처럼, LoRA도 **파라미터와 샘플 면에서 매우 효율적**

### 왜 LoRA가 효과적일까?
#### LoRA와 저랭크 분해의 배경 요약
- LoRA는 가중치 행렬을 저랭크 분해(A·B) 형태로 표현하고, 파인튜닝 시 원래 가중치 W는 고정한 채 A와 B만 학습하는 방식이다.
- 이는 사전학습된 모델이 이미 **낮은 내재적 차원(intrinsic dimension)**을 갖는 표현 공간에 수렴해 있다는 관찰에 기반한다.
- 즉, 잘 사전학습된 LLM일수록 소수의 학습 파라미터와 적은 데이터만으로도 효과적인 파인튜닝이 가능하다.

#### 왜 사전학습에는 LoRA를 쓰지 않을까?
- 많은 연구자들이 저랭크 분해를 사전학습 단계에도 적용해 학습 비용을 줄이려 시도해 왔다.
- CNN에서는 저랭크 분해가 성공적으로 적용된 사례가 많으며
(예: SqueezeNet은 매우 적은 파라미터로도 높은 성능 달성)
- 하지만 LLM 사전학습에서는 여전히 플랭크(full-rank) 학습이 필요하다는 점이 지배적이다.
- 사전학습 자체가 모델의 내재적 차원을 줄이는 역할을 하기 때문

#### 최근 연구 흐름
- **ReLoRA**: 최대 약 130억 파라미터 규모 트랜스포머에서 저랭크 학습 시도
- **GaLore**:
   - 10억 파라미터급에서는 플랭크 모델과 유사한 성능
   - 70억 파라미터급에서도 유망한 결과 보고
   - 향후에는 초대형 모델에서도 저랭크 사전학습이 가능해질 가능성이 있음

### LoRA 구성
- LoRA는 주로 트랜스포머 모델에 사용되고 보통 어텐션 모듈의 네 가지 가중치 행렬에 적용된다. => Query, Key, Value 그리고 출력 투영($W_o$) 행렬이다.
- 보통은 모델 내에서 같은 종류의 모든 행렬에 LoRA를 일괄적으로 적용한다.
   - ex) 쿼리 행렬에 LoRA를 적용한다면,모델 안의 모든 쿼리 행렬에 동일하게 적용하는 식
- 경험을 바탕으로 **feedforward 행렬**을 포함해 더 많은 가중치 행렬에 LoRA를 적용할수록 더 좋은 결과를 얻을 수 있다.
![](https://velog.velcdn.com/images/dkan9634/post/aa66ccea-bd00-475e-bfa5-daca1ad324dd/image.png)

- 성능이 랭크에 따라 달라지긴 하지만 **4~64 정도의 작은 r값만으로도 대부분의 활용 사례에서 충분하다.**
- r이 작을수록 LoRA 파라미터가 줄어들고, 메모리 사용량도 적어진다.
- r이 너무 높으면 과적합 때문에 오히려 성능이 떨어질수도 있지만실험마다 다르기 때문에 해보길 추천!
- 또 다른 LoRA 하이퍼파라미터는 병합 과정에서 $W_{AB}$ 곱이 새 행렬에 얼마나 영향을 미칠지 결정하는 $a$ 값이 있다.
$$
W' = W + \frac{a}{r}W_{AB}
$$
=> 실무에서는 a:r 비율을 보통 1:8에서 8:1 사이로 설정한다.(최적 비율은 경우에 따라 다르니 여러 실험 해보길)

### LoRA 어댑터 서빙
- LoRA는 더 적은 메모리와 데이터를 사용해서 모델을 파인튜닝할 수 있게 해주면서, **모듈성 덕분에** 여러 모델 서빙도 간단해진다.
- 모델 서빙 방법 2가지
1. 파인튜닝된 모델을 서빙하기 전에 LoRA 가중치 A, B를 원본 모델에 미리 병합해서 새로운 행렬 $W'$을 만든다. 추론할 때 추가 연산이 필요하지 않아서 지연 시간도 늘어나지 않는다.
   - 서빙할 LoRA 모델이 하나라면 첫 번째 방법이 더 좋다.

2. 서빙시 W, A, B 가중치를 각각 따로 유지한다. 이 경우 추론 과정에서 A와 B를 W에 병합해야 하므로 지연 시간이 늘어난다.
   - 멀티 LoRA 서빙에 더 좋다.
   - 저장 공간을 절약할 수 있다.

![](https://velog.velcdn.com/images/dkan9634/post/63c0d232-a6ce-447e-b7f2-a4a1f1394550/image.png)

-> LoRA 어댑터를 분리해서 유지할 경우 멀티 LoRA 서빙이 어떻게 이루어지는지 보여준다.

### 양자화된 LoRA
- 다양한 LoRA 변형들이 개발되었는데 이 중 일부는 학습 가능한 파라미터 수를 더 줄이는 것을 목표로 한다.
- 아래 표를 보면 LoRA 어댑터가 사용하는 메모리는 모델 가중치에 비해 매우 작다는 것을 알 수 있는데, 따라서 **LoRA 파라미터를 줄여봤자 전체 메모리 사용량은 거의 줄어들지 않는다.**
![](https://velog.velcdn.com/images/dkan9634/post/f442a712-e33d-413e-bfcf-974fb5967b03/image.png)

- LoRA의 파라미터 수를 줄이는 것보단, 파인튜닝할 때 모델의 가중치, 활성화 그래디언트를 양자화하는 편이 메모리를 훨씬 효과적으로 절약할 수 있다.
- 메모리를 절약할 수 있다는 장점 덕분에 양자화된 LoRA는 활발히 연구가 진행중이다.
   - `QLoRA`, `QA-LoRA`, `ModuLoRA`, `IR-QLoRA`
   

---
## 모델 병합과 다중 작업 파인튜닝
- 파인튜닝이 하나의 모델을 수정해서 맞춤형 모델을 만드는 방법이라면, **모델 병합**은 여러 모델을 결합해 맞춤형 모델을 만드는 방법이다.
- 파인튜닝만으로는 얻을 수 없는 더 큰 유연성을 가진다.
- 모델 병합 자체는 **GPU 없이** 할 수 있다
- 핵심은 개별 모델들을 따로따로 쓰는 것보다 훨씬 유용한 하나의 통합 모델을 만드는 데 있다.
- **메모리 사용량을 줄여서 비용을 절약할 수 있다.**
   - 서로 다른 작업 담당인 두 모델을 더 적은 파라미터로 두 작업을 모두 처리하는 하나의 모델로 통합할 수 있다.
   - 어댑터 기반 모델에서 특히 효과적
   - 같은 기본 모델에서 파생된 두 파인튜닝 모델이 있다면, 각각의 어댑터를 하나로 합칠 수 있기 때문이다.
   
- 모델 병합의 대표적인 사례: **다중 작업 파인튜닝**
- 만약 모델 병합 기법 없이 여러 작업에 대해 모델을 파인튜닝하려면 **동시 파인튜닝**이나 **순차 파인튜닝** 중 하나를 선택해야 한다.
- 스마트폰, 노트북, 자동차, 스마트워치, 창고 로봇 같은 기기에 모델을 배포할 때 모델 병합이 유리 => **온디바이스 배포**

#### 앙상블과 차이
- 모델 병합이 일반적으로 모델의 파라미터를 섞어서 만드는 방식이라면, 앙상블은 구성 모델을 그대로 놔둔 모델 출력만 결합하는 방식이다.
- 앙상블을 세 개의 모델을 사용한다고하면 단순 다수결 투표나 따로 학습된 ML 모듈을 통해 최종 답을 도출한다. => 요청 하나 당 여러 번 추론을 돌려야 해서 비용이 더 든다.

![](https://velog.velcdn.com/images/dkan9634/post/bd639794-bb4d-4555-a92d-c244f96c0cb2/image.png)


모델 병합 방법들은 **각 모델의 파라미터를 결합하는 방식에서 차이**가 난다.
=> **합산, 레이어 쌓기, 연결**
![](https://velog.velcdn.com/images/dkan9634/post/19ca6ef7-a757-44a6-bdb0-b60017d98bd8/image.png)

### 합산
- 이 접근법은 **구성 모델들의 가중치 값들을 더하는 방식**
- 두 가지 합산 방법이 있다.
   - 선형 결합, 구면 선형 보간법
#### 선형 결합
- 평균과 가중평균을 모두 포함한다.
- 두 모델 A, B가 주어졌을 때, 이들의 가중 평균은 다음과 같다.
![](https://velog.velcdn.com/images/dkan9634/post/5939b776-3a68-442c-9602-4be3589cbc96/image.png)
- $W_A = W_B = 1$일 때 두 레이어를 선형 결합하는 방법
![](https://velog.velcdn.com/images/dkan9634/post/c1488564-0bc4-47ea-b49b-082c53c03377/image.png)

- 같은 기본 모델에서 파인튜닝된 모델들끼리 결합할 때 가장 효과가 좋다.

#### 구면 선형 보간법(SLERP)
- 같은 이름의 수학 연산자인 구면 선형 보간법에서 가져온 것이다.
   - 보간은 알고 있는 값들을 바탕으로 모르는 값을 추정하는 방법
   
### 불필요한 파라미터 가지치기
- 모델 성능에 도움이 안되는 조정은 불필요한 것으로 간주된다.
- 작업 벡터 파라미터의 상당 부분을 재설정해도 성능이 거의 떨이지지 않는다는 논문이 있다.
![](https://velog.velcdn.com/images/dkan9634/post/8e0e79b0-dfc2-43dd-a537-255db6ddaff7/image.png)

- 불필요한 파라미터들은 개별 모델 단위에서는 딱히 해가 되지 않지만, 여러 모델을 병합 했을 땐 문제가 될 수 있다.
- **병합할 모델이 많을수록 가지치기가 더 중요해지는데, 한 작업의 불필요한 파라미터가 다른 작업들을 방해할 가능성이 그만큼 커지기 때문이다.**

### 레이어 쌓기
- 여러 모델에서 서로 다른 레이어를 가져다가 차곡차곡 쌓아올리는 방법
- 다른 말로 패스스루, 프랑켄머징
- 이렇게 하면 **기존에 없던 고유한 아키텍처와 파라미터 수를 가진 모델**을 만들 수 있다.
   - 합산 방식과는 다르게, 레이어 쌓기로 만든 모델은 제대로 된 성능을 내려면 보통 **추가 파인튜닝이 필요**하다.
- **전문가 혼합(MoE)** 모델을 학습할 떄도 활용할 수 있다.
   - MoE를 밑바닥부터 학습시키는 대신, 사전 학습된 모델을 가져와 특정 레이어나 모듈을 여러 개 복사한다. 
   - 그런 다음 라우터를 추가해서 각 입력을 가장 적합한 복사본으로 보낸다.
   - 그 다음 병합된 모델과 라우터를 함께 추가 학습시켜서 성능을 개선한다.
   ![](https://velog.velcdn.com/images/dkan9634/post/c232e69a-38c6-4d2d-92f2-48c1a7515502/image.png)

- 레이어 쌓기의 재밌는 활용법 중 하나는 **모델 업스케일링**
   - 적은 자원으로 더 큰 모델을 만드는 방법을 연구하는 분야
- 레이어 업스케일링 방법 중 하나는 **뎁스와이즈 스케일링(depthwise scaling)**
   - 32개의 레이어를 가진 7B 파라미터 모델 하나로 SOLAR 10.7B를 만들었다.
   - 기존 모델을 두 개 복사한 뒤, 일부 레이어는 합치고 나머지는 쌓아서 더 깊은 모델을 만든 다음, 추가 학습으로 성능을 회복하는 과정을 거쳤다.
![](https://velog.velcdn.com/images/dkan9634/post/8ac9152b-3453-42a5-a5af-49036825d707/image.png)

### 연결
- 그냥 연결하는 방법이다.
- 병합된 구성 요소의 파라미터 개수 = 모든 구성 요소의 파라미터를 다 합친 것
- 랭크가 x, y인 LoRA 어댑터 두 개를 병합하면, 병합된 어댑터의 랭크는 x+y가 될 것이다.
- 메모리 사용량이 전혀 줄지 않아 별로 추천하지 않는다.
- 연결 방식으로 성능이 더 좋아질 순 있지만, 늘어나는 파라미터 수를 생각하면 그 정도 성능 향상은 가치가 없을 수도 있다.

![](https://velog.velcdn.com/images/dkan9634/post/cb064994-6ce6-448d-83ff-1de98de7c61f/image.png)


## 파인튜닝 전술
더 실용적인 파인튜닝 노하우를 다뤄보자

### 파인튜닝 프레임워크와 기본 모델
- 파인튜닝 자체는 그리 복잡하지 않고 결정해야 할 건 딱 3가지이다.
=> **기본 모델, 파인튜닝 방법, 파인튜닝 프레임워크**

#### 기본 모델
- 프로젝트 초기에는 예산 범위 내에서 가장 강력한 모델부터 시작하는 것이 좋다.
→ 이 모델로도 성능이 안 나오면 더 작은 모델은 당연히 어렵기 때문.
- 처음에 강한 모델이 요구사항을 만족하면, 이후 더 저렴한 모델로 단계적으로 내려가며 비교하면 된다.
- 모델 선택 기준은 모델 크기, 라이선스, 벤치마크 성능 등이다.

- 파인튜닝 시작 전략 2가지
1. **진행 경로 (Progression Path)**
- 작은 모델 → 큰 모델로 확장하며 실험
- 가장 저렴하고 빠른 모델로 파인튜닝 파이프라인이 제대로 동작하는지 확인
- 중간급 모델로 데이터와 학습 설정이 적절한지 검증
- 최고 성능 모델로 성능 한계를 탐색
- 여러 모델의 비용 대비 성능을 비교해 최종 모델 선택

2. **증류 경로 (Distillation Path)**
- 큰 모델 → 작은 모델로 지식 이전
- 작은 데이터셋 + 가장 강력한 모델로 먼저 고성능 모델을 학습
- 해당 모델을 사용해 추가 학습 데이터 생성
- 생성된 데이터로 더 저렴한 모델을 학습

#### 파인튜닝 방법
- LoRA 같은 어댑터 기법은 비용 효율적이지만 보통 전체 파인튜닝만큼 성능이 나오지 않는다.
- 어떤 파인튜닝 방법을 사용할진 데이터 양에 따라서도 달라진다.
- 전체 파인튜닝은 보통 최소 수천 개의 예시가 필요하고, 대개는 훨씬 더 많이 필요하다.
   - 반면 PEFT 방법들은 훨씬 적은 데이터로도 괜찮은 성능을 낼 수 있다.
   - 수백 개 정도의 작은 데이터셋이 있다면, 전체 파인튜닝이 더 안좋을수도 있다.
- **파인튜닝 방법을 정할 땐 파인튜닝된 모델이 몇 개나 필요한지, 어떻게 서빙하고 싶은지도 생각**해봐야 한다.
   - LoRA 같은 어댑터 방법은 같은 기본 모델을 공유하는 여러 모델을 훨씬 더 효율적으로 서빙할 수 있다.
   - **LoRA를 사용하면 전체 모델 하나만 서빙하면 되지만, 전체 파인튜닝은 전체 모델을 여러 개 서빙해야 한다.**
   
#### 파인튜닝 프레임워크
- 파인튜닝을 가장 쉽게 하는 방법: 파인튜닝 API 쓰기
- `LlaMA-Factory`, `unsloth`, `PEFT`, `Axolotl`, `LitGPT` 같은 파인튜닝 프레임워크 중 하나를 사용해서 파인튜닝할 수도 있다.
- 직접 파인튜닝을 하면 더 많은 유연성을 얻을 수 있지만, 필요한 컴퓨팅 자원을 직접 준비해야 한다. 
- 여러 머신을 사용해서 모델을 파인튜닝 하려면, `딥스피드`, `PyTorch Distributed`, `ColossalAI` 같은 분산 학습을 도와주는 프레임워크가 피요할 것이다.

#### 파인튜닝 하이퍼파라미터
- 학습률
- 배치 크기
- 에폭 수

#### 프롬프트 손실 가중치
- 지시 파인튜닝에서는 프롬프트와 응답 모두 손실(loss)에 포함될 수 있다.
- 하지만 실제 사용 시에는 사용자가 프롬프트를 제공하고 모델은 응답만 생성하므로,
학습에서도 응답 토큰을 더 중요하게 다루는 것이 바람직하다.
- 프롬프트 손실 가중치는 프롬프트가 손실 계산에 얼마나 반영될지를 조절하는 값이다.
   - 100%: 프롬프트와 응답을 동일하게 학습
   - 0%: 응답만 학습
- 일반적으로 **약 10%**로 설정하며, 이는 프롬프트 정보는 일부 반영하되 주로 응답 품질 향상에 집중하기 위함이다.

> 프롬프트 손실 가중치는 학습 시 프롬프트보다 응답을 더 중요하게 학습하도록 조절하는 장치다.
---
[study-note](https://velog.io/@dkan9634/AI-Engineering-Chap-7.-%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D)

